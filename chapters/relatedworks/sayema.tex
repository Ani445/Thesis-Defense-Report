
\section{Weakly Supervised Semantic Segmentation}
\label{sec:weakly-supervised}
Fully supervised semantic segmentation depends on pixel-level segmentation masks annotated by humans. However, generating such dense annotations is tedious, time-intensive, and costly. Furthermore, crowd-sourced annotators must undergo special training to handle the complexity of pixel-level labeling, which restricts the scale and diversity of available datasets. Consequently, most curated datasets are limited to a small set of object categories. In contrast, unlabeled or weakly annotated images can be collected in abundance, at much lower cost and in shorter time. This has motivated research into weakly supervised semantic segmentation (WSSS) to make semantic segmentation models more scalable.


\section{Types of WSSS}
\label{sec:types-weakly-supervised}
A wide range of weak supervision have been explored, including bounding boxes, scribbles, points, image-level labels,  eye tracks, free-form squiggles, or noisy web tags. Bounding boxes provide rough object boundaries, offering useful localization cues, though they still require annotators to draw accurate boxes.  Scribble-based supervision allows annotators to roughly mark object regions without outlining exact object boundaries. Point supervision, by contrast, typically uses a single annotated pixel per object, giving coarse location information. While less costly than pixel-accurate masks, these methods still involve some level of manual annotation, making large-scale labeling expensive.


\section{Image-level Label based weak supervision}
\label{sec:image-level-label}
Image-level annotation is one of the most economical and efficient settings for weakly supervised semantic segmentation. In this context, every training image has its image class labels belonging to the objects present in the image but the locations of the objects are unknown. So, the challenge is to accurately assign image-level labels to their corresponding pixels.
 
Initial approaches attempted to train segmentation models directly from image-level labels \cite{dcnn}, but performance was unsatisfactory. Later methods introduced discriminative localization techniques such as Class Activation Maps (CAMs) \cite{cam}, which highlight class-relevant regions. These coarse cues were then refined using auxiliary information such as superpixels \cite{imagelevelpixel}, segmentation proposals \cite{imagelevelpixel}, or motion information from videos \cite{wsss_motion_cues}. Some works, such as Adversarial Erasing \cite{adversarial_erasing}, expanded object coverage progressively by iteratively searching new regions. Others, like Kolesnikov and Lampert \cite{kolesnikov2016}, trained networks to approximate the dense CRF \cite{krähenbühl} applied on CAMs for refinement.

Some approaches learn to predict affinity matrices at the pixel level [36], to refine the output of dCRF through random walk. AffinityNet \cite{wsss_affinitynet} predicts class-agnostic pixel affinities to propagate CAM activations via random walks. Similarly, Seeded Region Growing (SRG) \cite{srg} expands initial seed regions using similarity criteria, while its deep learning extension DSRG \cite{wsss_dsrg_deep_seeded_region_growing} leverages high-level semantic features to grow regions more effectively. P2P \cite{pixel_to_prototype} further narrows the supervision gap by using pixel-to-prototype contrastive learning. A common limitation, however, is that most of these CNN-based methods inherit the restricted locality of convolutional features.

Recent advances incorporate transformers into WSSS \cite{camtokens, getam}. TS-CAM \cite{camtokens} leverages the global information capturing ability of ViT by combining class-token attention with CAMs. \cite{getam} refines class-specific maps by using attention gradients. MCTformer \cite{wsss_MCTformer} expands this idea by embedding multiple class tokens to learn attention maps for different categories. AFA \cite{wsss_afa_affinity_from_attention} instead derives semantic affinities directly from attention maps to refine coarse pseudo-labels. FrozenCLIP \cite{wsss_frozen_clip} pushes this further by exploiting the frozen CLIP backbone to generate high-quality pseudo-labels, which are dynamically updated using a refinement module (RFM).


\section{Stages}
\label{sec:stages}

Weakly supervised semantic segmentation has two main solutions based on their training processes: multi-stage approaches \cite{instance_wsss,wsss_L2G,wsss_rib} and single-stage approaches \cite{wsss_reliability_does_matter, wsss_afa_affinity_from_attention}. 

\subsection{Multi Stage}
\label{subsec:multi-stage}

Multi Stage WSSS has multiple steps. Typically, a classification model is first trained to generate the CAM,  which are then converted into initial pixel-level pseudo labels. These pseudo labels are refined through affinity matrix and random walk propagation\cite{wsss_affinitynet, wsss_afa_affinity_from_attention}, or seeded region growing[SRG] or contrastive learning. Then these refined pseudo labels are used for supervising a segmentation model.

Du et al. \cite{pixel_to_prototype} introduced a pixel-to-prototype contrastive method that enforces semantic consistency at the feature level, leading to improved pseudo-label quality. MCTformer \cite{wsss_MCTformer} extended transformer-based models with multiple class tokens, enabling the generation of category-specific attention maps for refined CAMs. More recently, researchers have incorporated CLIP into the WSSS pipeline. For instance, CLIMS \cite{wsss_clims} used CLIP to highlight more complete object regions while suppressing confident background activations. Similarly, CLIP-ES \cite{wsss_clip_es} applied a softmax-based GradCAM \cite{cam_grad} guided by carefully designed text prompts, allowing CLIP to produce reliable pseudo labels for segmentation supervision.

\subsection{Single Stage}
\label{subsec:single-stage}

In single-stage weakly supervised semantic segmentation, the model tries to learn segmentation directly from weak supervision (like image-level labels) in one shot. A network is trained that produces segmentation outputs without going through separate phases of label generation and refinement. There’s no explicit intermediate step to improve pseudo labels — the model directly predicts segmentation maps during training based on weak supervision signals.

Earlier works in this line used ImageNet-pretrained backbones \cite{dataset_imagenet} to jointly optimize classification and segmentation, with most efforts devoted to improving supervision quality or constraining the learning process. AA\&AR \cite{wsss_aaar} proposed an adaptive affinity loss that facilitates semantic propagation within the segmentation branch. AFA \cite{wsss_afa_affinity_from_attention} introduced an affinity branch to refine CAMs online, yielding stronger pseudo labels during training. ToCo \cite{wsss_toco_token_contrast} further advanced this direction by employing token-level contrastive learning to mitigate over-smoothing in CAM generation, thereby providing better online supervision.

\section{Types of CAM}
\label{sec:types-of-cam}
CNNs are often regarded as “black-box” models due to the limited interpretability of their internal mechanisms. Zhou et al.\cite{cam} demonstrated that different CNN layers could function as unsupervised object detectors through Class Activation Mapping (CAM). Building on this, Grad-CAM\cite{cam_grad} combines the class-discriminative property of CAM with gradient-based visualization techniques to focus on fine-grained image details.

An extension of this method, Grad-CAM++~\cite{cam_gradpp}, improves both object localization and the handling of multiple object instances within the same image compared to prior approaches. It achieves this by assigning pixel-wise weights to the gradients of the output with respect to specific spatial locations in the final convolutional feature map. The method derives closed-form solutions for these weights, including exact higher-order derivatives for softmax and exponential activations. Importantly, Grad-CAM++~\cite{cam_gradpp} requires only a single backward pass, keeping computational cost comparable to earlier gradient-based methods while offering more informative visualizations.

LayerCAM~\cite{layer_cam} further advances interpretability by generating reliable activation maps not only from the final convolutional layer but also from shallower layers. This allows it to capture both coarse object localization and detailed fine-grained features, thereby offering a multi-scale perspective of object representation within CNNs.

Beyond CNN-based methods, attention-based interpretability has also been explored. \cite{attention_rollout}introduced attention rollout and attention flow to compute attention scores for input tokens at each layer. Both approaches recursively propagate attention from earlier layers, taking into account residual connections. Attention rollout assumes that token identities propagate linearly through attention weights, while attention flow models the process as a flow network and applies a maximum flow algorithm to compute maximum flow values and trace information flow from hidden embeddings to input tokens. Compared to raw attention maps, these methods provide token-level attention that shows stronger correlation with input-gradient-based importance scores.

RelevanceCAM~\cite{relevance_cam} introduces a new CAM variant that leverages Layer-wise Relevance Propagation (LRP) to compute weighting components. By doing so, it addresses the shattered gradient problem, which often leads to noisy saliency maps in intermediate layers. RelevanceCAM\cite{relevance_cam} demonstrates that not only the final convolutional layers but also intermediate and shallow layers, with smaller receptive fields, can capture class-specific features. This results in improved faithfulness and robustness of explanations, with superior localization of target objects in earlier layers compared to existing CAM-based techniques.

\section{Breakdown of the pipeline}
\label{sec:pipeline-breakdown}
\subsection{Backbone}
\label{subsec:backbone}

The architecture of AffinityNet \cite{wsss_affinitynet} relies on three DNNs: a classification network for generating CAMs, the AffinityNet module itself, and a segmentation network. All three share the same backbone. The backbone is a modified variant of Model A1 \cite{RevisitingResNET}, commonly referred to as ResNet38, which consists of 38 convolutional layers with wider channels. In this adaptation, the GAP and fully connected layers from the original design are removed, and the final three convolutional stages are replaced with atrous convolutions with stride 1. The dilation rates are adjusted so that the resulting feature maps maintain a stride of 8.

In DSRG \cite{wsss_dsrg_deep_seeded_region_growing}, the classification branch employs a slightly altered version of the 16-layer VGG model \cite{VGG16}, while the segmentation branch is built upon DeepLab-V2 \cite{fsss_deeplabv2}. Both are initialized with VGG-16 weights pre-trained on ImageNet.

AFA \cite{wsss_afa_affinity_from_attention} adopts the Mix Transformer (MiT) backbone introduced in SegFormer \cite{fsss_segformer}, which is more suitable for image segmentation tasks compared to the vanilla ViT. The MiT parameters are initialized using ImageNet-1k pre-trained weights.

ToCo \cite{wsss_toco_token_contrast} uses the ViT-Base (ViT-B) backbone initialized with ImageNet pre-trained weights. Within the ViT encoder, an auxiliary classification head is introduced to produce CAMs. These auxiliary CAMs are then used to form pseudo labels guiding the Pixel-Token Contrast (PTC) module, and also to generate proposals for cropping positive and negative local patches for the Cross-Token Contrast (CTC) module. The final CAM is generated through the classification layer and subsequently transformed into pseudo labels.

CLIP-ES \cite{wsss_clip_es} employs the CLIP ViT-B/16 backbone, where the image encoder extracts visual features and the text encoder extracts linguistic features. Since CLIP is pre-trained on roughly 400 million image-text pairs, it provides strong multimodal representations for segmentation.

The whole framework of FrozenCLIP \cite{wsss_frozen_clip} consists of four major components: a frozen CLIP backbone (comprising a ViT-Base/16 image encoder and a text encoder) \cite{transformer_vit}, a classification module for generating initial CAMs, a decoder for segmentation prediction, and a refinement module (RFM) to enhance CAMs into pseudo labels for supervision.

\subsection{CAM generation}
\label{subsec:cam-generation}
In AffinityNet \cite{wsss_affinitynet}, the CAM generation model has the following three layers added on the top of the backbone network: a 3x3 convolution layer with 512 channels to adapt to the target task, a global average pooling layer to aggregate feature maps, and a fully connected layer for classification. The class activation map is obtained by weighting the feature maps according to the corresponding class-specific weights in the FC layer.  These activation maps are normalized so that the maximum activation equals 1, and the background map is derived by subtracting the highest class activation of each pixel from 1.

In DSRG \cite{wsss_dsrg_deep_seeded_region_growing}, CAMs \cite{cam} are used to localize foreground objects.  In their classification network, the fully-connected classifier is applied to conv7 to generate a heatmap for each object class. Then the discriminative object regions, used as seed cues for the foreground, are obtained by applying a hard threshold to the heatmap. For localizing background seed cues, they selected the regions in normalized saliency maps whose pixels are with low saliency values as background.

AFA \cite{wsss_afa_affinity_from_attention} also relies on CAMs to provide initial pseudo labels. The maps are computed using the weighted contributions of feature maps from the classification layer, followed by a ReLU activation to suppress negative values. The outputs are scaled to the range [0,1], with an additional background score introduced to separate foreground from background.

In ToCo \cite{wsss_toco_token_contrast}, an auxiliary classification layer is added to extract semantic knowledge and generate CAMs. Here, patch tokens are first aggregated using global max pooling (GMP) and then passed through a fully connected layer, producing auxiliary CAMs that guide the learning process.

For FrozenCLIP \cite{wsss_frozen_clip}, image features are extracted using the frozen CLIP image encoder, while class-specific text prompts (foreground and background) are encoded via the CLIP text encoder. Both image and text encoders are frozen during training.  Then, the classification scores are generated by computing distances between pooled image features and text features. Based on classification scores, Grad CAM \cite{cam_grad} is utilized to generate the initial CAM.


\subsection{Pseudo Label Generation}
\label{subsec:pseudo-label-generation}
AffinityNet predicts convolutional feature maps in which the semantic affinity between two feature vectors is measured using their L1 distance. To train this network, semantic affinity labels are derived from CAMs. Specifically, confident foreground and background regions are identified, while ambiguous regions are treated as neutral. Pairwise affinities are then defined according to class labels — assigned as 1 for pixels of the same class, 0 for different classes, and ignored for neutral labels. Once trained, AffinityNet refines CAMs through a random walk guided by the semantic transition matrix, thereby enhancing CAM quality and producing more accurate pseudo segmentation labels.

In DSRG, pseudo labels are generated by expanding initial seed cues into unlabeled regions using the classical Seeded Region Growing (SRG) algorithm \cite{srg}. For each class, the seed cues are visited in row first manner and its 8-connectivity neighborhood pixels are checked for similarity criteria. If they satisfy the criteria , those pixels are added to the set of seed cues for that particular class, and this process is repeated for every object class. The dynamically updated seed sets act as supervision, continuously refining pseudo labels during training.

The AFA module computes semantic affinities directly by linearly combining multi-head attention outputs through an MLP layer. To ensure matrix symmetry, the affinity matrix is summed with its transpose, under the assumption that nodes with the same semantics should be equivalent. Supervision is provided by pseudo affinity labels derived from refined CAMs: pixels are assigned to the class with the highest activation, and affinities are set positive if two pixels share the same class, otherwise negative. These pseudo affinity labels supervise the affinity prediction process. The learned semantic affinities are then used in a random walk propagation to refine the CAMs.

In ToCo, auxiliary CAMs are segmented into pseudo token labels by applying two background thresholds, categorizing tokens into reliable foreground, background, and uncertain regions. Positive token pairs are defined as those sharing the same label, while others are treated as negative. To counteract over-smoothing, the Patch Token Contrast (PTC) module maximizes similarity between positive pairs of patch tokens and minimizes it for negative ones. Additionally, the Class Token Contrast (CTC) module enforces consistency across entire object regions by aligning representations of global and local class tokens. Specifically, it reduces the gap between projected global class tokens and projected local tokens cropped from uncertain or background regions, thereby improving pseudo label quality.

\subsection{Segmentation prediction}
\label{subsec:segmentation-prediction}
In AffinityNet, the segmentation model is constructed by adding two atrous convolution layers on top of the backbone. The segmentation predictions are then supervised using refined CAMs, which are generated by propagating pairwise pixel affinities.
For DSRG, once seed cues are obtained from initial CAMs, an image semantic segmentation network is trained with these cues. A balanced seeding loss is applied so that the network’s predictions are encouraged to match only the seed regions defined by the classification network, while ignoring unlabeled pixels. As training progresses, the seed cues are iteratively refined into pseudo labels, and the segmentation model is retrained with the updated supervision.
In ToCo, the segmentation decoder is deliberately simple, consisting of two 3×3 convolutional layers (each with dilation rate 5) followed by a 1×1 prediction layer. The pseudo labels produced by ToCo are passed through a Pixel-Adaptive Refinement (PAR) module, and the improved labels serve as supervision for the decoder to generate the final segmentation output.
For AFA, an MLP-based decoder head is employed. This head fuses features across multiple levels through lightweight MLP layers to produce segmentation predictions. The supervision is provided by refined pseudo labels obtained through affinity propagation.
In FrozenCLIP, segmentation is guided by a feature decoder that extracts intermediate outputs from each transformer block of the CLIP image encoder. For each feature map, a dedicated MLP is applied to generate enhanced feature representations, which are concatenated and then passed through a convolutional layer to form a fused feature map. This fused representation is subsequently processed by multiple stacked multi-head transformer blocks, producing the final segmentation prediction P.