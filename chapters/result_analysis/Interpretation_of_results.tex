
\section{Interpretation of Results}
\label{sec:interpretation_of_results}
The quantitative and qualitative analyses together provide a comprehensive understanding of the strengths and limitations of our weakly supervised segmentation framework.

From the quantitative perspective, the model achieves a mean IoU of 50.3\% on the validation set and 50.8\% on the test set, indicating strong generalization despite using only weak supervision. The per-class IoU breakdown (Table~\ref{tab:per_class_iou_comparison}) reveals that large and distinctive object categories—such as \textbf{bus}, \textbf{sheep}, \textbf{horse}, and \textbf{cat}—are segmented with high accuracy, while classes that are small, highly deformable, or poorly represented during pretraining (e.g., \textbf{person}, \textbf{chair}, and \textbf{potted plant}) show significantly weaker performance. This pattern aligns with the biases inherent in the pretraining datasets (ImageNet-21K and YFCC-14M), which lack dense, diverse coverage for these semantic categories.

Qualitative inspection further supports these findings. Visual examples show that predictions for animals and vehicles tend to produce coherent and complete masks with clear object boundaries, whereas human instances often exhibit fragmented or overly diffuse activations. In particular, the \textbf{person} class consistently fails due to its initial Class Activation Maps (CAMs) being dominated by high confidence in the \textbf{background} regions. Since the Refinement Module (RFM) operates via random walk propagation, these early misaligned activations are spread across spatially adjacent pixels, reinforcing incorrect background predictions. Moreover, the pixel-adaptive module, while effective in smoothing object interiors, tends to over-suppress fine structural details in thin or articulated objects such as limbs, further degrading segmentation quality for humans.

Overall, the results indicate that our model is highly effective in identifying large, contextually distinct objects but remains sensitive to CAM initialization errors and class imbalance during pretraining. The analysis suggests that improving CAM reliability—particularly for human-centric and small-object categories—through better initialization, adaptive refinement control, or scale-aware feature fusion could significantly enhance overall segmentation quality.