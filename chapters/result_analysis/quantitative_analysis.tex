\section{Quantitative Analysis}
\label{sec:quantitative_analysis}

We evaluated the performance of our proposed method on the PASCAL VOC 2012 \textbf{validation} and \textbf{test} sets using standard metrics: \textbf{mean Intersection over Union (mIoU)}, \textbf{pixel accuracy (pAcc)}, and \textbf{mean accuracy (mAcc)}. The results are summarized in Table~\ref{tab:summarized_results}. 

On the \textbf{validation set}, our method achieves a \textbf{mean IoU of 50.3\%}, a pixel accuracy of 80.1\%, and a mean accuracy of 69.99\%.  
On the \textbf{test set}, the model achieves a slightly higher \textbf{mean IoU of 50.8\%}, with a pixel accuracy of 79.6\% and mean accuracy of 71.36\%.  

These results indicate that the model generalizes well across both validation and test sets, consistently segmenting dominant classes effectively. In particular, the strong performance on large and distinctive categories such as \textbf{background}, \textbf{bus}, \textbf{sheep}, and \textbf{cow} highlights the model's ability to capture prominent visual structures. However, performance remains weaker on small or less frequent categories such as \textbf{person}, \textbf{chair}, and \textbf{bicycle}, suggesting that future improvements should focus on enhancing recognition of rare and small objects.

\input{chapters/result_analysis/per_class_performance.tex}

\input{chapters/result_analysis/comparison_with_baselines.tex}


%summary
\subsection{Summary of Quantitative Analysis}
\label{subsubsec:quantitative_summary}

Overall, the quantitative results reveal that our UniCL-AffSeg framework demonstrates reasonable segmentation ability under image- and language-level supervision, despite operating without pixel-level annotations. The model achieves comparable performance on both validation (50.3\% mIoU) and test (50.8\% mIoU) sets, indicating good generalization and stable behavior across unseen data. 

A closer look at per-class IoUs shows that UniCL-AffSeg performs strongly on large, visually distinct categories such as \textit{bus}, \textit{cow}, and \textit{sheep}, while struggling on small or underrepresented ones like \textit{person}, \textit{chair}, and \textit{bicycle}. This disparity highlights the limitations of Swin-based affinity representations in capturing fine-grained or globally contextual features. When compared with state-of-the-art weakly supervised segmentation models, our approach trails methods like WeCLIP and ToCo, which rely on ViT-based attention mechanisms. The relatively lower mIoU suggests that while Swin Transformers provide strong local representations, their limited global context impedes effective region propagation during pseudo-label generation. 

These findings collectively emphasize the need for integrating global reasoning components—such as cross-window attention or hybrid ViT-Swin architectures—to bridge the gap with ViT-based WSSS models and improve small-object segmentation consistency.

