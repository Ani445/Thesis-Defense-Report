\chapter{Proposed Methodology}
\label{chap:methodology}

In our proposed methodology, we aim to leverage the strengths of both the UniCL framework and the Swin Transformer architecture to enhance the performance of our model. The methodology is structured into several key components, each addressing a specific aspect of the overall process. The following sections provide a detailed overview of each component, including the backbone architecture, CAM generation, pseudo label generation, segmentation prediction, and refinement.

\section{Overview}
\label{sec:overview}
In this section, we provide a high-level overview of the proposed methodology. The methodology is designed to address the challenges of weakly supervised semantic segmentation (WSSS) by leveraging the strengths of multi-modal learning frameworks like UniCL and advanced vision architectures such as the Swin Transformer. The key components of the methodology include:

\begin{itemize}
    \item \textbf{Backbone Architecture}: The backbone network is responsible for generating Class Activation Maps (CAMs) from input images. We experiment with UniCL as the backbone due to its multi-modal capabilities and state-of-the-art performance in generating high-quality CAMs.
    \item \textbf{CAM Generation}: CAMs are generated using Grad-CAM techniques adapted for multi-modal models like UniCL. These CAMs highlight the regions of interest in the image for each class.
    \item \textbf{Segmentation Prediction}: The feature maps from the backbone are aggregated and passed through a decoder to produce segmentation predictions. The decoder employs multi-head transformer layers to refine the predictions.
    \item \textbf{CAM Refinement and Pseudo-Label Generation}: Initial CAMs are refined using affinity-based techniques and random walk propagation to generate high-quality pseudo-labels. These pseudo-labels serve as supervision for training the segmentation model.
    \item \textbf{Loss Function}: A combination of segmentation loss and affinity loss is used to optimize the model. The affinity loss ensures consistency between the affinity maps of the decoder and the pseudo-labels, while the segmentation loss aligns the predictions with the refined CAMs.
\end{itemize}

The proposed methodology integrates these components into a cohesive framework, enabling the generation of accurate segmentation maps from weak supervision signals.

\section{Background}
\label{sec:background}

In this section, we will discuss the background of the proposed methodology, including the concepts of Class Activation Maps (CAMs), the backbone architecture, and the process of generating pseudo labels. We will also provide an overview of the UniCL framework and its components, as well as the Swin Transformer architecture and its advantages for image segmentation tasks.

\subsection{Class Activation Maps (CAMs)}

Class Activation Maps (CAMs) are a powerful tool for visualizing the regions of an image that contribute most to the model's predictions. They provide insights into the model's decision-making process and help identify the areas of interest for segmentation tasks. In this section, we will discuss the process of generating CAMs using the UniCL framework and Swin Transformer architecture.

\subsection{Basic Classification}
\label{subsec:basic_classification}

In the basic classification task, the model is trained to classify images into predefined categories. The CAMs are generated by applying a global average pooling operation on the feature maps produced by the last convolutional layer of the model. This operation aggregates the spatial information and produces a heatmap that highlights the regions of the image that are most relevant for the classification task.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{basic-classification}}
    \caption{Basic Classification}
    \label{fig:basic_classification}
\end{figure}

In the \autoref{fig:basic_classification}, we can see a high-level overview of the basic classification task. The process can be summarized as follows:

\begin{enumerate}
    \item The input image is passed through the model, and the feature maps are generated. This deep feature extraction block can be either CNNs (as it initially was) or Transformers.
    \item The \emph{MLP (Multi-Layer Perceptron)}, also called the \emph{Classifier head}, is used to produce the final classification scores.
    \item The scores are then normalized using a \emph{softmax} function to obtain the class probabilities.
\end{enumerate}

No matter how the feature maps are generated or what the backbone is, the MLP head is used to produce the final classification scores. For CAM generation, we only need the feature maps and the class scores, i.e, upto the Classifier head.

\subsection{CAM Generation Process}
\label{subsec:cam_generation_process}

For any type of CAM generation, the classification step is a must. The classification step is the same as the basic classification task, but we need to tweak it a bit depending on the type of CAM we are generating.

\subsubsection{Basic CAM}

For example, as shown in\autoref{fig:basic_cam_generation_process}, the basic CAM generation process is as follows (described in \cite{cam}):
\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{basic-CAM}}
    \caption{CAM Generation Process}
    \label{fig:basic_cam_generation_process}
\end{figure}

\begin{enumerate}
    \item The input image is passed through the model, and the feature maps are generated.
    \item A GAP (Global Average Pooling) operation is applied to the feature maps to obtain an aggregated image representation.
    \item The GAP output is then passed through the Classifier head to produce the final classification scores.
    \item The neuron of MLP corresponding to the highest score, or the predicted class, is selected.
    \item The weights of the selected neuron are used to compute the weighted sum of the feature maps, resulting in a heatmap that highlights the regions of the image that contribute most to the predicted class.
    \item The heatmap is then upsampled to the original image size to visualize the regions of interest.
\end{enumerate}

\subsubsection{Grad-CAM}
\label{subsec:grad_cam_generation_process}
On the other hand, Grad-CAM (Gradient-weighted Class Activation Mapping) is a more advanced technique that uses the gradients of the predicted class with respect to the feature maps to generate the CAM. The process is as follows (described in \cite{cam_grad} and illustrated in \autoref{fig:grad_cam_generation_process}):

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{grad-CAM}}
    \caption{Grad-CAM Generation Process}
    \label{fig:grad_cam_generation_process}
\end{figure}

\begin{enumerate}
    \item The input image is passed through the model, and the feature maps are generated.
    \item The Classifier head is used to produce the final classification scores.
    \item The neuron of MLP corresponding to the highest score, or the predicted class, is selected.
    \item The gradients of the selected neuron with respect to the feature maps are computed. This step is crucial as it captures the importance of each feature map for the predicted class.
    \item The average of the gradients is taken across the spatial dimensions to obtain the weights for each feature map.
    \item The weights are used to compute the weighted sum of the feature maps, resulting in a heatmap that highlights the regions of the image that contribute most to the predicted class.
    \item The heatmap is then passed through a ReLU activation function to retain only the positive contributions.
    \item The heatmap is upsampled to the original image size to visualize the regions of interest.
\end{enumerate}

\subsection{Class selection}
\label{subsec:class_selection}
In the basic classification task, the predicted class is determined by selecting the class with the highest score from the Classifier head. However, for CAM generation, we have the flexibility to generate a CAM for any specific class of interest, not just the predicted class. For instance, if an image contains both a cat and a dog, and the dog has the highest score, we can still generate the CAM for the "cat" class by selecting the corresponding neuron from the Classifier head. This allows us to visualize the regions of the image relevant to the selected class, even if it is not the top prediction.

This flexibility is a key point WSSS. In WSSS, an image is provided along with the labels for all classes present in the image. The model is trained to generate CAMs for each of these classes individually. These CAMs are then refined and used to create pseudo-labels, which serve as supervision for the segmentation task.

\subsection{Multi-Modal Learning}
\label{subsec:multi_modal_learning}
Multi-modal learning refers to the process of learning from multiple sources of information, such as images and text. This approach allows models to leverage complementary information from different modalities, leading to improved performance on various tasks. For example, in classification tasks, the ability to learn from both images and text enables the model to capture richer semantic information, enhancing its understanding of the data. More importantly, it can enable zero-shot capabilities, where the model can classify images it has never seen before. This is achieved by associating the visual features of an image with its textual description, allowing the model to generalize to unseen classes. As classification is often a foundational step in many processes, this ability is highly valuable.

\subsubsection{Contrastive Learning}
\label{subsec:contrastive_learning}
Contrastive learning is a self-supervised learning approach that aims to learn representations by contrasting positive and negative pairs. This involves aligning features from related inputs (e.g., an image and its corresponding text description) in a shared embedding space while ensuring that unrelated features are pushed apart. A contrastive loss function is typically used to minimize the distance between positive pairs while maximizing the distance between negative pairs. This approach helps models learn discriminative and meaningful representations that are robust and generalizable.

\subsubsection{Training Strategy}
\label{subsec:training_strategy}
The training strategy for multi-modal learning often involves a combination of supervised and self-supervised learning techniques. Models are trained on large-scale datasets containing paired samples from different modalities, enabling them to learn meaningful representations through techniques like contrastive learning. Data augmentation is frequently employed to improve the robustness and diversity of the learned features, ensuring better generalization to unseen data.

\subsubsection{Task-Agnostic Learning}
\label{subsec:task_agnostic_learning}
Task-agnostic learning refers to the ability of a model to learn representations that are not specifically tailored to a single task but can be applied across various tasks. This is achieved by focusing on learning generalizable features that are beneficial for multiple downstream applications. By training on diverse datasets and leveraging multi-modal information, models can develop representations that are adaptable to different tasks, such as image classification, object detection, and image segmentation. This flexibility makes task-agnostic learning a powerful approach for building versatile and reusable models.

\subsection{Contrastive Language-Image Pre-Training (CLIP)}
\label{subsec:clip}

CLIP \cite{vl_clip} is a multi-modal learning framework that leverages contrastive learning to align images and text in a shared embedding space. It is designed to learn representations that can generalize across various tasks without requiring task-specific fine-tuning. CLIP achieves this by training on a large dataset of image-text pairs, allowing it to learn meaningful relationships between visual and textual information.

\subsection{CLIP Architecture}
\label{subsec:clip_architecture}

\subsubsection{CLIP Components}
CLIP consists of two main components:

\begin{itemize}
    \item \textbf{Image Encoder}: This component is responsible for processing images and extracting features. It typically includes a swin transformer, pooling layers, and other modules designed to capture spatial hierarchies and relationships within the image data.
    \item \textbf{Text Encoder}: This component processes text data, converting it into a format suitable for comparison with image features. It often includes transformer layers and attention mechanisms to capture the semantic relationships within the text.
\end{itemize}

The learning of CLIP is achieved through a contrastive loss function that aligns the image and text features in a shared embedding space. This allows the model to learn meaningful representations that can be applied to various tasks, such as image classification, object detection, and image segmentation. Or, in our case, weakly supervised semantic segmentation, which requires only image-level classification.

\subsection{Unified Contrastive Learning (UniCL)}
\label{subsec:unicl}

UniCL, short for Unified Contrastive Learning \cite{vl_unicl}, is another multi-modal learning framework that builds upon the principles of CLIP. It aims to unify various contrastive learning approaches into a single framework, allowing for more efficient and effective learning from multiple modalities.

But it is not just a simple extension of CLIP. UniCL introduces several key innovations that enhance its performance and versatility.

First, UniCL employs a unified architecture that combines image-label and text-label contrastive learning into a single framework, making a image-label-text space. This allows the contrastive learning process to be more efficient and effective. Because, in the image-text space, in contrastive learning, the model learns to increase the similarity between the image and text features while decreasing the similarity between unrelated features. CLIP used to assume the image-text pair, that was provided as input, to be the only positive pair. But in reality, there are many other positive pairs in the dataset. For example, if we have an image of a cat and the text "a photo of a cat" is provided as input, then another similar text, "a photo of a kitten" can also be a positive pair. CLIP does not consider this. but UniCL does. It tries to learn the similarity between the image and all the text features that have the same category as the image. This is done by using a contrastive loss function that encourages the model to learn a shared representation space for both images and text.

\subsubsection{Unified Image-Text-Label Contrast in UniCL}
\label{subsec:unified_image_text_label_contrast}

UniCL employs a bidirectional learning objective between image-text pairs:
\begin{equation} \label{eq:unified_image_text_label_contrast}
    \min_{\{\theta, \phi\}} \mathcal{L}_{\text{BiC}} = \mathcal{L}_{i2t} + \mathcal{L}_{t2i},
\end{equation}
where \(\mathcal{L}_{i2t}\) and \(\mathcal{L}_{t2i}\) are the image-to-text and text-to-image contrastive losses, respectively. And \(\theta\) and \(\phi\) are the parameters of the image and text encoders, respectively.

The image-to-text contrastive loss to align matched images in a batch with a given text is defined as:
\begin{equation} \label{eq:unicl_image_to_text_contrastive_loss}
    \mathcal{L}_{i2t} = - \sum_{i \in \mathcal{B}} \frac{1}{|\mathcal{P}(i)|} \sum_{k \in \mathcal{P}(i)} 
    \log \frac{\exp(\tau \mathbf{u}_i^\top \mathbf{v}_k)}{\sum_{j \in \mathcal{B}} \exp(\tau \mathbf{u}_i^\top \mathbf{v}_j)},
\end{equation}
where $k \in \mathcal{P}(i) = \{k | k \in \mathcal{B}, y_k = y_i\}$, i.e., the set of all images in the batch that belong to the same class as image $i$.

On the other hand, the text-to-image contrastive loss to align matched text in a batch with a given image is defined as:

\begin{equation} \label{eq:unicl_text_to_image_contrastive_loss}
    \mathcal{L}_{t2i} = - \sum_{j \in \mathcal{B}} \frac{1}{|\mathcal{P}(j)|} \sum_{k \in \mathcal{P}(j)} 
    \log \frac{\exp(\tau \mathbf{u}_k^\top \mathbf{v}_j)}{\sum_{i \in \mathcal{B}} \exp(\tau \mathbf{u}_i^\top \mathbf{v}_j)},
\end{equation}
where $k \in \mathcal{P}(j) = \{k \mid k \in \mathcal{B}, y_k = y_j\}$, i.e., the set of all text features in the batch that belong to the same class as text $j$.

\subsubsection{Comparison with CLIP}
\label{subsec:clip_vs_unicl}

In case CLIP, for an image, there is only one positive text feature. In other words, $\mathcal{P}(i) = {i} \in \mathcal{B}$; and $\mathcal{P}(j) = {j} \in \mathcal{B}$. So, the image-to-text contrastive loss is defined as:

\begin{equation} \label{eq:clip_image_to_text_contrastive_loss}
    \mathcal{L}_{i2t} = - \sum_{i \in \mathcal{B}} 
    \log \frac{\exp(\tau \mathbf{u}_i^\top \mathbf{v}_i)}{\sum_{j \in \mathcal{B}} \exp(\tau \mathbf{u}_i^\top \mathbf{v}_j)},
\end{equation}
where $i \in \mathcal{B}$ is the index of the image in the batch.

And the text-to-image contrastive loss is defined as:

\begin{equation} \label{eq:clip_text_to_image_contrastive_loss}
    \mathcal{L}_{t2i} = - \sum_{j \in \mathcal{B}} 
    \log \frac{\exp(\tau \mathbf{u}_j^\top \mathbf{v}_j)}{\sum_{i \in \mathcal{B}} \exp(\tau \mathbf{u}_i^\top \mathbf{v}_j)},
\end{equation}
where $j \in \mathcal{B}$ is the index of the text feature in the batch. 

The equations \ref{eq:unified_image_text_label_contrast}-\ref{eq:clip_text_to_image_contrastive_loss} are taken from \cite{vl_unicl}.

This means that $\mathcal{L}_{BiC}$ becomes CLIP training objective. The main property in \autoref{eq:unicl_text_to_image_contrastive_loss} is that for each image feature, any of the text features in the batch can be used as a positive pair. And so is the case for \autoref{eq:unicl_text_to_image_contrastive_loss}.

Second, UniCL replaces CLIP's ViT backbone with a Swin Transformer backbone. The Swin Transformer is a hierarchical vision transformer that captures both local and global information in images, making it more suitable for various vision tasks.

\subsection{Swin Transformer}
\label{subsec:swin_transformer}
Swin Transformer is a hierarchical vision transformer that employs a shifted windowing scheme to capture both local and global information in images. It is designed to be computationally efficient while maintaining high performance on various vision tasks. The architecture consists of multiple stages, each with its own set of transformer blocks, allowing it to process images at different resolutions and capture multi-scale features.

\autoref{fig:swin_vs_vit_architecture} illustrates the architectures of the Swin Transformer and Vision Transformers (ViT). The key distinction lies in their processing strategies: the Swin Transformer employs a hierarchical structure, processing feature maps at multiple resolutions, while ViT processes the entire image globally at a single resolution. This hierarchical approach enables the Swin Transformer to effectively capture both local and global information, making it particularly suitable for tasks requiring fine-grained details alongside overall context.

The Swin Transformer builds hierarchical feature maps by progressively merging image patches (depicted in gray) in deeper layers. It computes self-attention only within local windows (highlighted in red), resulting in linear computational complexity relative to the input image size. This efficiency, combined with its ability to handle dense recognition tasks, makes it a versatile backbone for both image classification and segmentation.

In contrast, traditional vision Transformers, such as ViT, produce feature maps at a single low resolution and compute self-attention globally, leading to quadratic computational complexity with respect to the input image size. This global attention mechanism, while effective for certain tasks, is less efficient for dense prediction tasks compared to the Swin Transformer's localized attention mechanism.

\begin{figure}[htbp]
    \centering
    \fbox{ % Draw a box around the entire figure
        \begin{minipage}{0.9\textwidth} % Adjust the width as needed
            \begin{subfigure}[b]{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{swin-vs-vit-archi-a.pdf}
                \caption{Swin Transformer}
                \label{fig:swin_transformer_data_flow}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{swin-vs-vit-archi-b.pdf}
                \caption{Vision Transformers}
                \label{fig:vit_data_flow}
            \end{subfigure}
        \end{minipage}
    }
    \caption{Comparison of Swin Transformer and ViT Architectures}
    \label{fig:swin_vs_vit_architecture}
\end{figure}

Because of the hierarchical nature of the Swin Transformer, it is particularly well-suited for tasks that require a combination of global context (to indentify the overall structure of the image) and local details (to capture fine-grained features, e.g., edges, textures). This makes it a strong candidate for image segmentation tasks.

Also, the Contrastive and Multi-Modal Learning capabilities of UniCL can be effectively combined with the Swin Transformer's hierarchical architecture to enhance the model's ability to learn rich and discriminative features from both image and text modalities. This combination allows for a more comprehensive understanding of the data, leading to improved performance.

Now, getting all the background information, we can proceed to the proposed methodology. The proposed methodology consists of several key components, including the backbone architecture, CAM generation, pseudo label generation, segmentation prediction, and refinement. Each component plays a crucial role in enhancing the overall performance of the model.

\begin{figure}
    \centering
    \fbox{\includegraphics[width=0.9\textwidth]{architecture.png}}
    \caption{architecture}
    \label{fig:architecture}
\end{figure}

\section{Backbone}
\label{sec:backbone}

As we have seen so far, the pseudo labels and CAMs are two critical components of WSSS. First of all, the CAMs are generated from the backbone network, using which the pseudo labels are generated. The pseudo labels are then used to train the segmentation head (or segmentation model in case of multi-stage approach).

So the more accurate the initial CAMs are, the better the pseudo labels will be. And so will be the segmentation model. So we need to ensure that the CAMs are as accurate as possible. Keeping this in mind, we are experimenting with different backbone architectures to find the one that generates the most accurate CAMs.

So, we have chosen UniCL as the first one to experiment with. The reason is that it is a multi-modal model, and it has been shown to produce high-quality CAMs. It is also a state-of-the-art model for zero-shot image classification. So, we can expect it to produce high-quality CAMs.

\section{CAM Generation}
\label{sec:cam_generation}

\autoref{fig:cam_generation_process_clip} shows the process of generating CAMs using CLIP/UniCL. We have used Grad-CAM. The process is similar to Grad-CAM, as discussed in \autoref{subsec:grad_cam_generation_process}.

AS CLIP is a multi-modal model, the main classification task is to find similarity between the image and text.

If we want to generate the CAM for $N$ classes, we need to provide $N$ text features as input. So, at the very beginning, we need to encode the text features using the CLIP text encoder, and store them.

An MLP layer basically takes the linear combination of the input features and the weights. We do the same thing when finding similarity score between the image and text features, take the linear combination of the image features and the text features. So, we can consider these text features as the "neuron weights" of the classifier head.

Then, we need to pass the image through the CLIP image encoder to get the feature maps. The rest of the process is the same as Grad-CAM.


\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{clip-grad-cam.pdf}}
    \caption{CAM Generation Process using CLIP}
    \label{fig:cam_generation_process_clip}
\end{figure}

\section{Segmentation Prediction}
\label{sec:segmentation_prediction}
The segmentation prediction module is designed to generate pixel-wise class predictions for the input image. It consists of two main components: encoder feature aggregation and a decoder with multi-head transformer layers. It is constructed following \cite{wsss_frozen_clip}.

\subsection{Encoder Feature Aggregation}
\label{subsec:en_feature_agg}
Following \cite{wsss_frozen_clip}, the feature maps from the transformer blocks of the UniCL \cite{vl_unicl} image encoder need to be passed to the decoder. Let the feature maps be \( F_l^{\text{init}} \) for \( l = 1, \dots, N \) from the output of each transformer block in the encoder, where \( l \) represents the index of the block. Then, for each feature map \( F_l^{\text{init}} \), an individual MLP module is used to generate new corresponding feature maps \( F_l^{\text{new}} \):
\begin{equation}
    F_l^{\text{new}} = W_{1}^{\text{fc}} \left( \text{ReLU} \left( W_{2}^{\text{fc}}(F_l^{\text{init}}) \right) \right),
\end{equation}
where \( W_{1}^{\text{fc}} \) and \( W_{2}^{\text{fc}} \) are two different fully-connected layers, and \(\text{ReLU}(\cdot)\) is the ReLU activation function.


After that, all new feature maps \( F_l^{\text{new}} \) for \( l = 1, \dots, N \) are concatenated together, which are then processed by a convolution layer to generate a fused feature map \( F_u \):
\begin{equation}
    F_u = \text{Conv}\left( \text{Concat}\left[ F_1^{\text{new}}, F_2^{\text{new}}, \dots, F_N^{\text{new}} \right] \right),
    \tag{2}
\end{equation}

where \( F_u \in \mathbb{R}^{d \times h \times w} \), and \( d \), \( h \), and \( w \) represent the channel dimension, height, and width of the feature map, respectively. \(\text{Conv}(\cdot)\) is a convolutional layer, and \(\text{Concat}[\cdot]\) denotes the concatenation operation.


\begin{figure}[t]
    \centering
    \fbox{\includegraphics[width=0.9\textwidth]{decoder.png}}
    \caption{Decoder architecture}
    \label{fig:decoder}
\end{figure}

\subsection{Generating Final Prediction}
\label{subsec:decoder_final_pred}

Several sequential multi-head transformer layers are designed to generate the final prediction \( P \):
\begin{equation}
    \label{eq:prediction}
    P = \text{Conv}(\phi(F_u)) \uparrow,
\end{equation}
where \( P \in \mathbb{R}^{C \times H \times W} \), \( C \) is the number of classes including background, and \(\phi\) represents the sequential multi-head transformer blocks [12]. Each transformer block contains a multi-head self-attention module, a feed-forward network, and two normalization layers. The operator \(\uparrow\) denotes an upsampling operation to align the prediction map size with the original image.


\section{CAM Refinement and Pseudo-Label Generation}
\label{sec:refinement}
\subsection{Affinity based CAM refinement}
The initial CAMs produced by the backbone tend to focus only on sparse, highly discriminative regions. It is often noisy and insufficient to be used for supervision. Therefore, a refinement module is required where the initial CAM is improved. The refined CAM is the pseudolabel that is used as supervision.

The most common method of recitifying CAM is by using the feature relationships of the backbone that produces it. This is called affinity-based CAM refinement, first proposed by \cite{wsss_affinitynet}. We know that attention maps of a transformer capture the semantic-level affinity of the features or patches. The affinity matrix $A_f$ is produced from these attention maps which is used to refine the CAMs using random walk propagation. Another affinity matrix, $\hat{A}$ is generated from the predicted pseudolabel by the decoder. Then the CE loss between these two affinity matrices are incorporated into the loss function.

However, in our case, the backbone that produces the CAM is frozen, and therefore, the attention maps of the backbone will not get updated during training. To remedy this, what Frozen Clip \cite{wsss_frozen_clip} does is - it utilizes the affinity map of intermediary features of the decoder to refine the CAMs. While doing so, the attention maps of the backbone are used to influence this decoder affinity matrix in order to produce the final affinity matrix, $R$ , which is then used to generate the transition matrix, $T$ for the random walk propagation. The complete process is described below.

\subsection{Extracting affinity map from the decoder}
\label{subsec:decoder_aff_mat}
Based on the feature map $F_u$ obtained from the decoder (as defined in Eq.~(2)), an affinity map is constructed as follows:

\begin{equation}
    \label{eq: A_f}
    A_f = \text{Sigmoid}(F_u^\top F_u),
\end{equation}

where $F_u \in \mathbb{R}^{d \times h \times w}$ is reshaped into a matrix of size $d \times hw$ before the computation. The $\text{Sigmoid}(\cdot)$ function ensures that the values in the resulting affinity map lie in the range $[0, 1]$. Consequently, the computed affinity map $A_f$ has the dimensions $\mathbb{R}^{hw \times hw}$. Here, $\top$ denotes the matrix transpose.

\subsection{Selecting attention maps of the image encoder}
\label{subsec:att_map_encoder}
We extract all the multi-head attention maps from the frozen CLIP image encoder, denoted as $\{A_s^l\}_{l=1}^N$, where each attention map $A_s^l \in \mathbb{R}^{hw \times hw}$. To evaluate the quality of each attention map $A_s^l$, we compare it with the previously computed affinity map $A_f$ using the following score:

\begin{equation}
    S^l = \sum_{i=1}^{hw} \sum_{j=1}^{hw} \left| A_f(i, j) - A_s^l(i, j) \right|,
\end{equation}

where $S^l$ measures the deviation of the attention map $A_s^l$ from the affinity map $A_f$.

We then compute a binary filter $G^l$ for each attention map based on its corresponding score:

\begin{equation}
    G^l =
    \begin{cases}
        1, & \text{if } S^l < \frac{1}{N - N_0 + 1} \sum_{l=N_0}^N S^l \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

Here, $G^l \in \mathbb{R}^{1 \times 1}$ is expanded to $G^l_e \in \mathbb{R}^{hw \times hw}$ for further processing. The threshold is set as the average of the $S^l$ values from layer $N_0$ to $N$. If a particular $S^l$ is less than this threshold, the corresponding attention map is considered reliable and retained ($G^l = 1$). Otherwise, it is deemed weak and filtered out ($G^l = 0$). This filtering process ensures that only high-quality attention maps are preserved for subsequent computations.

\subsection{Utilizing the attention maps}
\label{subsec: mul_attn_and_aff}
Using the affinity map $A_f$ and the filtered attention maps, we construct a refining map $R$ by weighing the affinity matrix with the mean of the selected attention maps. The equation is given as:

\begin{equation}
    R = A_f \odot \frac{ \sum_{l=1}^N G^l_e A_s^l}{N_m},
\end{equation}

where $\odot$ denotes element-wise multiplication, and $N_m$ is the number of valid attention maps that have passed the filtering stage, defined as:

\begin{equation}
    N_m = \sum_{l = N_0}^N G^l.
\end{equation}

Here, $G^l_e \in \mathbb{R}^{hw \times hw}$ is the expanded binary filter corresponding to $G^l$. The refining map $R$ effectively integrates the reliable attention maps weighted by the affinity map $A_f$, enhancing meaningful relationships while suppressing noisy or irrelevant information.

\subsection{Constructing the semantic transition matrix}
\label{subsec:trans_mat}
Now that we have the affinity matrix $A_f$, to apply the random walk propagation, we need to at first normalize the rows for converting it into a transition probability matrix. In addition, column normalization ensures consistency and symmetric behavior. Sinkhorn normalization \cite{math_sinkhorn} is thus used to convert $A_f$ into a double stochastic matrix $R_{\text{nor}}$.

Next, $R_{\text{nor}}$ is converted into a symmetric matrix by adding its transpose and normalizing the sum:

\[
    T = \frac{R_{\text{nor}} + R_{\text{nor}}^T}{2}, \quad \text{where} \quad R_{\text{nor}} = \text{Sinkhorn}(A_f)
\]

The matrix $T$ is now symmetric whose rows and columns are normalized. This is the transition probability matrix to be used for random walk.


\begin{figure}[tbp]
    \centering
    \fbox{\includegraphics[width=0.9\textwidth]{refinement.png}}
    \caption{CAM refinement by exploiting affinity map}
    \label{fig:refinement}
\end{figure}

\subsection{Random Walk Propagation}
\label{subsec:random_walk}

The refined class activation map (CAM) for a specific class $c$, denoted as $M^c_f$ is found by random walk propagation as follows:

\begin{equation}
    M_f^c = B^c \odot T^\alpha \cdot M_{\text{init}}^c
\end{equation}

where, $M^c_{\text{init}} \in \mathbb{R}^{hw \times 1}$ is the initial CAM for class $c$, reshaped into vector form. $T$ is the transition probability matrix, $\alpha$ is the hyperparameter that controls the strength of the refinement we want to apply. It is equivalent to the number of iterations of the random walk propagation. \( B^c \in \mathbb{R}^{1 \times hw} \) is the box mask obtained from the CAM of class \( c \), and \( \odot \) denotes the Hadamard product. This masking is needed to restrict the refining region spatially. It is obtained for each target class c by thresholding the CAM of this class by a constant. The connected regions of the mask map are found and covered by minimum rectangle bounding boxes. These boxes mask the affinity of distant pixels, preventing over-expansion.

Finally, the refined CAM $M^c_f$ is passed through an online post-processing module - specifically, the pixel-adaptive refinement module proposed in \cite{wsss_afa_affinity_from_attention}.

\subsection{Pixel Adaptive refinement module}
\label{subsec:par}
To ensure local consistency, pixel-adaptive convolution is introduced by \cite{wsss_afa_affinity_from_attention}. It uses local RGB and spatial information to define the low-level pair-wise semantic affinity of two pixels. Given an input image \( I \in \mathbb{R}^{h \times w \times 3} \), the pairwise affinities between two pixels at positions \((i, j)\) and \((k, l)\) is calculated as follows:

- \( \kappa^{\text{rgb}}_{ij,kl} \) measures color similarity (the closer the colors, the higher the affinity).

- \( \kappa^{\text{pos}}_{ij,kl} \) measures spatial closeness (pixels closer in space have higher affinity).

These are defined as:

\[
    \kappa^{\text{rgb}}_{ij,kl} = -\frac{\| I_{ij} - I_{kl} \|^2}{w_1 \sigma^2_{\text{rgb}}}, \quad
    \kappa^{\text{pos}}_{ij,kl} = -\frac{\| P_{ij} - P_{kl} \|^2}{w_2 \sigma^2_{\text{pos}}}
\]

Here:
- \( I_{ij} \) and \( P_{ij} \) are the RGB values and 2D spatial coordinates at pixel \((i, j)\),
- \( \sigma_{\text{rgb}} \) and \( \sigma_{\text{pos}} \) are the standard deviations of color and position differences,
- \( w_1 \) and \( w_2 \) are weights that control smoothness.

Then, the affinity kernel \( \kappa_{ij,kl} \) for each pixel pair is computed by applying a softmax to normalize both terms across the local neighborhood \( \mathcal{N}(i, j) \), and combining them:

\[
    \kappa_{ij,kl} = \frac{ \exp(\kappa^{\text{rgb}}_{ij,kl}) }{ \sum\limits_{(x, y) \in \mathcal{N}(i, j)} \exp(\kappa^{\text{rgb}}_{ij,xy}) }
    + w_3 \cdot \frac{ \exp(\kappa^{\text{pos}}_{ij,kl}) }{ \sum\limits_{(x, y) \in \mathcal{N}(i, j)} \exp(\kappa^{\text{pos}}_{ij,xy}) }
\]

Here, \( w_3 \) is a weight to balance the influence of the position term.

This affinity kernel is used to iteratively refine a CAM \( M \in \mathbb{R}^{h \times w \times C} \). At iteration \( t \), each pixel value \( M^t_{i,j,c} \) for class \( c \) is updated by aggregating class scores from neighboring pixels weighted by the affinity:

\begin{equation}
    M^t_{i,j,c} = \sum_{(k, l) \in \mathcal{N}(i, j)} \kappa_{ij,kl} \cdot M^{t-1}_{k,l,c}
\end{equation}


The neighborhood \( \mathcal{N}(i, j) \) is defined as the 8-connected neighbors (i.e., a \(3\times{3}\) window) with multiple dilation rates  allowing the refinement to capture both local and semi-local context.

\subsection{Pseudo-Label Generation}
\label{subsec:pseudo_label_generation}
The refined CAMs are used to generate the pseudo-labels or segmentation labels, which assign a class label to each pixel (patch),
\begin{equation}
    M_p(x, y) = \arg\max_{c \in \{1, \ldots, C\}} M_c(x, y)
\end{equation}
where \( M_c \in \mathbb{R}^{h \times w} \) is the refined CAM and \( M_p \in \mathbb{R}^{h \times w} \) is the pseudo-label.
The generated pseudo-labels are used as supervision for the segmentation network.



\section{Loss Function}
\label{subsec:loss_func}

The loss function defined by \cite{wsss_frozen_clip}:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{seg} + \lambda \mathcal{L}_{aff}
\end{equation}

where, $\mathcal{L}_{aff}$ is the affinity loss and $\mathcal{L}_{seg}$ is the segmentation loss. $\lambda$ is the weighting parameter.
\subsection{Affinity Loss}
\label{aff_loss}

In the refinement module, we used the affinity map of the decoder features, $A_f$ (\autoref{eq: A_f}). The quality of the final pseudolabel directly depends on it. We need to ensure that the pairwise affinity of the pseudolabel matches the pairwise affinity of the decoder output, because that is what the random walk propagation implies.
We compute the affinity labels for the predicted pseudolabel as follows:

\begin{equation}
    \hat{A} = O_h(M_p)^TO_h(M_p)
\end{equation}
where $O_h(.)$ is the one-hot encoding of $M_p$, and $A \in \mathbb{R}^{hw \times hw}$  is the affinity label.
. This means that the value of $\hat{A(i,j)}$ will be 1 for the $(i,j)$ pair which have the same class label, and 0 otherwise. The affinity loss is the Cross-Entropy Loss of $A_f$ and $\hat{A}$:
\begin{equation}
    \mathcal{L}_{aff} = \mathcal{L}_{ce}(A_f, \hat{A})
\end{equation}

\subsection{Segmentation Loss}
\label{seg_loss}
We obtain the segmentation prediction, $P$, from the decoder (\autoref{eq:prediction}). As we are using the refined CAM or pseudolabel, $M_p$ as supervision, the segmentation loss is computed:

\begin{equation}
    \mathcal{L}_{seg} = \mathcal{L}_{ce}(P, M_p \uparrow)
\end{equation}

where \( L_{ce} \) is the cross-entropy loss, and \( M_p{\uparrow} \in \mathbb{R}^{H \times W} \). $H$ and $W$ are the original height and width of the image respectively.


