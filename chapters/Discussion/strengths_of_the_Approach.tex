\section{Strengths of the Approach}
\label{sec:strengths_of_approach}

The proposed UniCL-AffSeg framework demonstrates several strengths that collectively advance the state of weakly supervised semantic segmentation (WSSS). By combining multi-modal representation learning with hierarchical transformer-based feature extraction and affinity-driven refinement, UniCL-AffSeg effectively enhances both localization accuracy and boundary quality in segmentation masks. The following subsections outline these major strengths.

\subsection{Multi-Modal Learning and Unified Objectives}
Leveraging UniCL’s unified contrastive and classification objectives, the framework aligns image and text representations in a shared embedding space. This integration enriches the semantic understanding of visual features without requiring pixel-level supervision. In practice, this results in more discriminative and spatially coherent Class Activation Maps (CAMs), as reflected in the model’s improved localization performance on large and visually distinctive categories such as \textit{bus}, \textit{sheep}, and \textit{horse}. The cross-modal synergy also enhances generalization across diverse scenes, reducing dependency on external localization cues like saliency maps.

\subsection{Hierarchical Feature Extraction with Swin Transformer}
The Swin Transformer backbone provides hierarchical multi-scale features that capture both local and global context—an essential quality for dense prediction tasks. Its shifted-window attention mechanism preserves fine-grained boundary details while maintaining computational efficiency. This hierarchical representation allows UniCL-AffSeg to delineate object boundaries more precisely and maintain structural continuity, particularly in multi-object scenes where traditional CAMs often suffer from fragmentation or missing parts.

\subsection{Affinity-Based Refinement for Semantic Consistency}
The affinity-based refinement module significantly enhances semantic consistency in pseudo-labels. By integrating encoder- and decoder-level affinity maps and filtering them through deviation scoring, the framework propagates relevant activations across object regions while suppressing background noise. This results in smoother, more complete segmentation masks with improved intra-object coherence. The approach effectively reduces over-segmentation and false activations in cluttered scenes, contributing to the framework’s robustness under purely weak supervision.

\subsection{Efficiency and Scalability}
Despite incorporating multi-modal and transformer-based components, UniCL-AffSeg remains computationally efficient. Swin’s linear-complexity attention and UniCL’s pre-trained representations enable scalable training on large datasets like PASCAL VOC 2012. Moreover, the Pixel-Adaptive Refinement (PAR) module introduces only minimal overhead while significantly improving boundary sharpness through adaptive color and spatial filtering. Together, these design choices ensure a favorable balance between segmentation accuracy and computational cost, making the framework suitable for broader real-world deployment.

\subsection{Overall Impact}
These strengths collectively enable UniCL-AffSeg to produce high-quality CAMs and pseudo-labels that bridge much of the gap between weakly supervised and fully supervised methods. The framework demonstrates that hierarchical transformers and multi-modal pretraining, when combined with affinity-based refinement, can deliver scalable and annotation-efficient segmentation without reliance on dense supervision.
