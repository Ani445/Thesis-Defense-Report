\section{Explored but Ineffective Approaches}
\label{sec:ineffective_approaches}

During development, we investigated several alternative strategies that, while promising in theory, did not yield satisfactory results in practice. We summarize these negative results here both for completeness and to highlight potential pitfalls for future work.  

\paragraph{Alternative CAM Generation Methods.} 
We experimented with LayerCAM and Grad-CAM++, as replacements for our primary CAM extraction mechanism. Although these methods are widely used in weakly supervised settings, in our framework they produced coarse and noisy activation maps, failing to capture complete object regions. Figure~\ref{fig:cam_comparison} provides a visual comparison, illustrating how these alternatives highlighted only small discriminative parts while missing large portions of the objects.  

\paragraph{Backbone Fine-tuning Variants.}
We also attempted to fine-tune the last one or two Swin Transformer blocks, both with and without an additional fully connected (FC) projection layer. These modifications degraded the quality of the generated CAMs, often resulting in unstable or inconsistent activations. This suggests that partial fine-tuning, without carefully designed regularization, may overfit to local patterns and disrupt the alignment with text embeddings.  

\paragraph{Local-to-Global Attention Conversion.}
Finally, we explored replacing Swinâ€™s window-based local attention with a global attention mechanism to improve long-range reasoning. However, this approach led to a sparse and ineffective global attention matrix, which in turn degraded segmentation quality. This outcome underscores the difficulty of naively extending Swin to global attention without appropriate architectural adaptations.  

In summary, although these explorations did not improve performance, they provide valuable insights. They demonstrate the limitations of directly adopting generic CAM methods, highlight the fragility of partial backbone fine-tuning, and reveal the challenges of balancing local and global attention in window-based Transformers.
