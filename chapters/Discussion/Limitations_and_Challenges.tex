\section{Limitations and Challenges}
\label{sec:limitations_and_challenges}

Despite its conceptual alignment with recent single-stage approaches such as WeCLIP, our UniCL-AffSeg framework currently exhibits several limitations that hinder its overall effectiveness.  

First, the reliance on the Swin Transformer as the image encoder introduces structural constraints. Swin employs a window-based self-attention mechanism, which provides strong local feature representations but lacks the global receptive field necessary to model holistic object-level semantics. As a result, the affinity maps extracted from Swin are predominantly local and often fail to capture long-range dependencies between distant regions of an image. This locality limits the ability of UniCL to propagate semantic information effectively across the entire object, leading to fragmented or incomplete predictions.  

Second, the initial class activation maps (CAMs) generated within UniCL tend to be weak and spatially inconsistent compared to ViT-based methods. Since CAM quality directly affects the downstream segmentation process, these weak initial cues significantly constrain the achievable performance, even with refinement mechanisms. Moreover, unlike attention maps from ViT backbones, which inherently encode some global semantic context, affinity maps from Swin offer limited interpretability and are more sensitive to noise, further exacerbating the challenge of producing robust segmentation masks.  

Third, the feature maps from the Swin image encoder, while rich in fine-grained local detail, lack the semantic alignment needed for reliable affinity learning when paired with the ViT text encoder. This modality mismatch introduces additional difficulties in bridging image–text correspondence, which is critical in weakly supervised scenarios where no dense pixel-level supervision is available.  

Another limitation of the current UniCL-AffSeg framework lies in the absence of multi-scale feature fusion. Many state-of-the-art WSSS approaches leverage multi-scale representations to capture objects of varying sizes and to integrate both fine-grained local cues and global contextual information. In contrast, our current design relies solely on single-scale features extracted from the Swin image encoder. Although Swin provides hierarchical feature maps across different stages, these maps exist at different spatial resolutions, preventing straightforward concatenation or fusion. The lack of multi-scale integration has two important consequences: (i) small or thin structures (e.g., object parts or fine boundaries) are often underrepresented in the generated CAMs, and (ii) large objects with spatially distributed regions are not consistently captured due to the limited receptive field of window-based attention. This partially explains the performance gap with methods such as WeCLIP that inherently benefit from the global receptive fields of ViT attention maps.  

Beyond these architectural constraints, broader practical challenges also remain. The framework requires gradient accumulation to compensate for limited GPU memory, which increases training time and slows down experimentation. The relatively small effective batch size can also introduce noisy gradient estimates, reducing convergence stability. In addition, the framework is sensitive to hyperparameter settings, such as learning rate schedules, number of iterations, and CAM refinement parameters. Suboptimal choices in these aspects can significantly degrade performance, which complicates adaptation to new datasets or domains.  

Finally, foreground–background separation remains a persistent challenge in weakly supervised semantic segmentation. Reliance on image-level or language-level cues often leads to incomplete coverage of object regions, while post-processing techniques such as CRF, although beneficial, cannot fully compensate for weak initial localization signals.  

In summary, UniCL-AffSeg highlights both the potential and the challenges of extending contrastive learning frameworks to weakly supervised segmentation. Addressing the locality of Swin’s attention, improving CAM quality, incorporating multi-scale feature fusion, reducing training sensitivity to hyperparameters, and developing strategies that integrate local affinity with global reasoning are key directions for overcoming these limitations.
