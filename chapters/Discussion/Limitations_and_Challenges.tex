\subsection{Limitations and Challenges}
\label{subsec:limitations_and_challenges}
Despite its conceptual alignment with recent single-stage approaches such as WeCLIP, our UniCL-AffSeg  framework currently exhibits several limitations that hinder its effectiveness. First, the reliance on the Swin Transformer as the image encoder introduces structural constraints. Swin employs a window-based self-attention mechanism, which provides strong local feature representations but lacks the global receptive field necessary to model holistic object-level semantics. As a result, the affinity maps extracted from Swin are predominantly local and often fail to capture long-range dependencies between distant regions of an image. This locality limits the ability of UniCL to propagate semantic information effectively across the entire object, leading to fragmented or incomplete predictions.  

Second, the initial class activation maps (CAMs) generated within UniCL tend to be weak and spatially inconsistent compared to ViT-based methods. Since CAM quality directly affects the downstream segmentation process, these weak initial cues significantly constrain the achievable performance, even with refinement mechanisms. Moreover, unlike attention maps from ViT backbones, which inherently encode some global semantic context, affinity maps from Swin offer limited interpretability and are more sensitive to noise, further exacerbating the challenge of producing robust segmentation masks.  

Third, the feature maps from the Swin image encoder, while rich in fine-grained local detail, lack the semantic alignment needed for reliable affinity learning when paired with the ViT text encoder. This modality mismatch introduces additional difficulties in bridging image–text correspondence, which is critical in weakly supervised scenarios where no dense pixel-level supervision is available.  

Another limitation of the current UniCL-AffSeg  framework lies in the absence of multi-scale feature fusion. Many state-of-the-art WSSS approaches leverage multi-scale representations to capture objects of varying sizes and to integrate both fine-grained local cues and global contextual information. In contrast, our current design relies solely on single-scale features extracted from the Swin image encoder. Although Swin provides hierarchical feature maps across different stages, these feature maps have different spatial resolutions, which prevents them from being directly concatenated for multi-scale fusion. The lack of multi-scale fusion has two important consequences: (i) small or thin structures (e.g., object parts or fine boundaries) are often underrepresented in the generated CAMs, and (ii) large objects with spatially distributed regions are not consistently captured due to the limited receptive field of the window-based attention mechanism. As a result, UniCL struggles to produce coherent masks across scales, which partially explains the performance gap with methods such as WeCLIP that inherently benefit from the global receptive fields of ViT attention maps.
 

Beyond these architectural constraints, broader challenges remain in weakly supervised semantic segmentation. Foreground–background separation is particularly difficult without pixel-level supervision, and reliance on image-level or language-level cues often results in incomplete coverage of object regions. Additionally, post-processing techniques such as CRF can provide improvements but cannot fully compensate for weak initial localization signals.  

In summary, UniCL-AffSeg  highlights both the potential and the challenges of extending contrastive learning frameworks to weakly supervised segmentation. Addressing the locality of Swin’s attention, improving the quality of initial CAMs, incorporating multi-scale feature fusion, and developing strategies to integrate local affinity with global reasoning are key directions for overcoming these limitations.
