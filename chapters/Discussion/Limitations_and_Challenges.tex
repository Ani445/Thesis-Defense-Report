% \section{Limitations and Challenges}
% \label{sec:limitations_and_challenges}

% Despite its conceptual alignment with recent single-stage approaches such as WeCLIP, our UniCL-AffSeg framework currently exhibits several limitations that hinder its overall effectiveness.  

% First, the reliance on the Swin Transformer as the image encoder introduces structural constraints. Swin employs a window-based self-attention mechanism, which provides strong local feature representations but lacks the global receptive field necessary to model holistic object-level semantics. As a result, the affinity maps extracted from Swin are predominantly local and often fail to capture long-range dependencies between distant regions of an image. This locality limits the ability of UniCL to propagate semantic information effectively across the entire object, leading to fragmented or incomplete predictions.  

% Second, the initial class activation maps (CAMs) generated within UniCL tend to be weak and spatially inconsistent compared to ViT-based methods. Since CAM quality directly affects the downstream segmentation process, these weak initial cues significantly constrain the achievable performance, even with refinement mechanisms. Moreover, unlike attention maps from ViT backbones, which inherently encode some global semantic context, affinity maps from Swin offer limited interpretability and are more sensitive to noise, further exacerbating the challenge of producing robust segmentation masks.  

% Third, the feature maps from the Swin image encoder, while rich in fine-grained local detail, lack the semantic alignment needed for reliable affinity learning when paired with the ViT text encoder. This modality mismatch introduces additional difficulties in bridging image–text correspondence, which is critical in weakly supervised scenarios where no dense pixel-level supervision is available.  

% Another limitation of the current UniCL-AffSeg framework lies in the absence of multi-scale feature fusion. Many state-of-the-art WSSS approaches leverage multi-scale representations to capture objects of varying sizes and to integrate both fine-grained local cues and global contextual information. In contrast, our current design relies solely on single-scale features extracted from the Swin image encoder. Although Swin provides hierarchical feature maps across different stages, these maps exist at different spatial resolutions, preventing straightforward concatenation or fusion. The lack of multi-scale integration has two important consequences: (i) small or thin structures (e.g., object parts or fine boundaries) are often underrepresented in the generated CAMs, and (ii) large objects with spatially distributed regions are not consistently captured due to the limited receptive field of window-based attention. This partially explains the performance gap with methods such as WeCLIP that inherently benefit from the global receptive fields of ViT attention maps.  

% Beyond these architectural constraints, broader practical challenges also remain. The framework requires gradient accumulation to compensate for limited GPU memory, which increases training time and slows down experimentation. The relatively small effective batch size can also introduce noisy gradient estimates, reducing convergence stability. In addition, the framework is sensitive to hyperparameter settings, such as learning rate schedules, number of iterations, and CAM refinement parameters. Suboptimal choices in these aspects can significantly degrade performance, which complicates adaptation to new datasets or domains.  

% Finally, foreground–background separation remains a persistent challenge in weakly supervised semantic segmentation. Reliance on image-level or language-level cues often leads to incomplete coverage of object regions, while post-processing techniques such as CRF, although beneficial, cannot fully compensate for weak initial localization signals.  

% In summary, UniCL-AffSeg highlights both the potential and the challenges of extending contrastive learning frameworks to weakly supervised segmentation. Addressing the locality of Swin’s attention, improving CAM quality, incorporating multi-scale feature fusion, reducing training sensitivity to hyperparameters, and developing strategies that integrate local affinity with global reasoning are key directions for overcoming these limitations.




\section{Limitations and Challenges}
\label{sec:limitations_and_challenges}

Despite its conceptual alignment with recent single-stage approaches such as WeCLIP, our UniCL-AffSeg framework reveals several limitations that help explain its underperformance in practice. These challenges can be grouped into architectural constraints, pseudo-label generation issues, and broader training difficulties.

\subsection{Architectural Constraints}

\paragraph{Locality of Swin Attention.}
The Swin Transformer employs a window-based self-attention mechanism, which provides strong local feature representations but lacks a truly global receptive field. While beneficial for capturing fine-grained details, this design prevents the model from reasoning holistically about objects spread across distant regions. Consequently, the affinity maps produced by Swin tend to be locally accurate but globally fragmented, limiting UniCL’s ability to propagate semantic information across the entire object. By contrast, ViT backbones in CLIP capture long-range dependencies more naturally through global self-attention, yielding more coherent semantic maps.

\paragraph{Feature–Text Modality Mismatch.}
Although UniCL was designed to unify contrastive and classification-based objectives, its integration with Swin features exposes a modality gap. Swin feature maps are hierarchical and locally focused, whereas the ViT-based text encoder provides globally contextualized embeddings. This structural mismatch weakens image–text correspondence, which is particularly detrimental in weakly supervised settings where no pixel-level annotations exist to bridge the gap. The result is unreliable affinity learning and degraded segmentation quality.

\paragraph{Lack of Multi-Scale Feature Fusion.}
State-of-the-art WSSS methods often leverage multi-scale representations to capture both small details and large object contexts. While Swin naturally produces hierarchical feature maps at different resolutions, our current framework processes these scales in isolation rather than integrating them. This omission leads to two shortcomings: (i) small or thin object parts are often overlooked due to insufficient fine-scale emphasis, and (ii) large objects are inconsistently segmented because their spatially distributed regions cannot be jointly reasoned about. In contrast, ViT-based approaches such as WeCLIP benefit from globally uniform attention maps that inherently integrate multiple scales of context.

\subsection{Pseudo-Label Generation Limitations}

\paragraph{Weak and Inconsistent CAMs.}
The quality of the initial class activation maps (CAMs) is critical for downstream segmentation. In our framework, CAMs generated from Swin features tend to be weaker and more spatially inconsistent than those derived from ViT backbones. Unlike ViT attention maps, which implicitly encode global semantic cues, Swin affinity maps are more sensitive to local noise and lack interpretability. This often results in excessive false positives and incomplete object coverage, constraining the effectiveness of refinement steps.

\paragraph{Foreground–Background Separation.}
A persistent challenge in WSSS is distinguishing objects from background clutter. Since our pseudo-labels rely heavily on weak CAMs and affinity propagation, background regions are frequently mislabeled as foreground, while parts of true objects remain unactivated. Although post-processing methods such as CRFs can partially alleviate this issue, they cannot fully compensate for noisy or incomplete initial cues.

\subsection{Practical Training Challenges}

\paragraph{Computational Limitations.}
Due to limited GPU memory, our framework requires gradient accumulation to achieve reasonable batch sizes. This increases training time and reduces the stability of optimization, as small effective batch sizes introduce noisy gradient estimates. The additional overhead slows down experimentation, making hyperparameter tuning less tractable.

\paragraph{Sensitivity to Hyperparameters.}
The UniCL-AffSeg framework exhibits high sensitivity to training schedules and refinement parameters. Factors such as learning rate decay, number of iterations, and CAM post-processing settings substantially influence final performance. This instability complicates adaptation to new datasets or domains and makes reproducibility more difficult compared to more stable ViT-based methods.

\subsection{Summary}

In summary, the underperformance of UniCL-AffSeg can be traced to a combination of architectural and practical factors: the locality bias of Swin attention limits global reasoning, the mismatch between Swin image features and ViT text embeddings undermines modality alignment, the absence of multi-scale fusion reduces robustness across object sizes, and the generated CAMs are weak and noisy. Together with resource and hyperparameter sensitivities, these factors explain the performance gap relative to ViT-based approaches such as WeCLIP. Addressing these challenges requires strategies that integrate Swin’s local strengths with mechanisms for global semantic reasoning, multi-scale fusion, and more reliable pseudo-label generation.

