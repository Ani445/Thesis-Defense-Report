\chapter{Conclusion}
\label{chap:conclusion}

This study presented UniCL-AffSeg, a weakly supervised semantic segmentation framework that integrates multi-modal contrastive learning with hierarchical transformer architectures to generate more accurate and spatially coherent Class Activation Maps (CAMs). Building upon UniCL's unified vision-language representations and Swin Transformer's hierarchical design, the proposed method effectively improves pseudo-label quality through affinity-based and pixel-adaptive refinement strategies.

Comprehensive experiments on the PASCAL VOC 2012 dataset demonstrated that UniCL-AffSeg achieves strong segmentation performance under purely weak supervision, with a mean IoU of 50.3 \% on the validation set and 50.8 \% on the test set. The framework excels for large, visually distinctive classes such as bus, sheep, and horse, while highlighting the persistent challenge of segmenting fine-structured or underrepresented categories like person and chair. These findings confirm the value of integrating multi-modal pretraining and affinity-driven refinement for scalable, annotation-efficient segmentation.

While the discussion chapter elaborated on the framework's strengths and limitations, the results collectively emphasize that improving CAM generation and bias mitigation in pretraining remain the most critical next steps toward closing the gap between weakly and fully supervised methods.

The following section outlines several promising directions for advancing this research—such as exploring diverse backbones, hybrid local-global reasoning, and enhanced prompt learning—which together provide a roadmap for future development in weakly supervised and multi-modal semantic segmentation.

\input{chapters/conclusion/future.tex}