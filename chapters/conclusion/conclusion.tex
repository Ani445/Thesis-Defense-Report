\chapter{Conclusion}
\label{chap:conclusion}

This study presented UniCL-AffSeg, a weakly supervised semantic segmentation framework that integrates multi-modal contrastive learning with hierarchical transformer architectures to produce spatially coherent and semantically meaningful Class Activation Maps (CAMs). By leveraging UniCL’s unified vision-language representations and the Swin Transformer’s hierarchical design, the proposed approach enhances pseudo-label quality through affinity-based and pixel-adaptive refinement strategies.

The experimental analyses demonstrate that UniCL-AffSeg performs competitively under purely weak supervision, effectively capturing large and visually distinctive objects while revealing persistent challenges in segmenting small or fine-structured categories. These outcomes confirm the benefit of combining multi-modal pretraining with affinity-driven refinement, offering a scalable and annotation-efficient alternative to fully supervised methods. Beyond quantitative performance, the findings underscore the importance of improving CAM generation and addressing pretraining biases to further close the gap between weakly and fully supervised segmentation. The framework illustrates how vision-language alignment can guide more structured pixel-level understanding from limited supervision.

The following section outlines potential directions for future exploration—such as incorporating hybrid local-global reasoning, adaptive prompt learning, and alternative transformer backbones—which together provide a roadmap for advancing weakly supervised and multi-modal semantic segmentation research.

\input{chapters/conclusion/future.tex}
