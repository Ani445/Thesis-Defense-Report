\section*{Future Work}
\label{sec:future}


So far, we have explored various models, architectures, and methods for refining CAMs, pseudo-labels, and segmentation maps. However, there remain many promising directions for future research and improvement. Below is a comprehensive list of potential future works, combining our previous ideas with additional suggestions from recent advances in WSSS, CLIP, and UniCL:

\begin{itemize}
    \item \textbf{Experiment with Diverse Backbones and Architectures:} Explore a wider range of backbone networks (e.g., ViT, ResNet variants, Mix Transformer, etc.) and decoder architectures to improve CAM and segmentation quality.
    \item \textbf{Advanced Feature Aggregation:} Develop improved methods for aggregating feature maps and attention maps from intermediate layers, such as transformer-based fusion modules or multi-scale feature alignment.
    \item \textbf{Enhanced CAM and Pseudo-Label Refinement:} Investigate new strategies for refining CAMs and pseudo-labels, including graph-based, CRF-based, or affinity-based refinement, as well as uncertainty-aware or region-based approaches.
    \item \textbf{Alternative Segmentation Models:} Evaluate different segmentation heads and decoders (beyond SegFormer) to assess their impact on weakly supervised segmentation performance.
    \item \textbf{Novel Objective Functions:} Explore new loss functions, such as region-based, contrastive, or uncertainty-aware objectives, to better guide the learning process.
    \item \textbf{Prompt Engineering for Multi-Modal Models:} Design and experiment with improved or learnable text prompts for CLIP/UniCL to enhance image-text alignment, especially for rare or ambiguous classes.
    \item \textbf{Self-Supervised and Semi-Supervised Pretraining:} Utilize large-scale self-supervised or semi-supervised pretraining to improve backbone representations before WSSS training.
    \item \textbf{Hybrid Local-Global Reasoning:} Combine local affinity (e.g., Swin Transformer) with global attention mechanisms (e.g., ViT) to improve semantic propagation across distant regions.
    \item \textbf{Cross-Dataset Generalization:} Evaluate and adapt models for better transferability across datasets with different distributions or label sets.
\end{itemize}

These directions provide a comprehensive roadmap for future exploration and improvement in weakly supervised semantic segmentation and multi-modal learning.