\chapter{Introduction}
\label{chap:introduction}

\section{Semantic Segmentation}
\label{sec:semantic_segmentation}
In recent years, computer vision has experienced remarkable progress, fueled by the availability of large annotated datasets, advances in computational power, and the development of sophisticated deep learning models. Within this field, semantic segmentation has gained significant importance as a challenging yet essential task with wide-ranging applications in areas such as autonomous driving, medical diagnostics, remote sensing, robotics, and augmented reality. Unlike traditional image classification, which assigns a single category to an entire image, semantic segmentation provides a fine-grained, pixel-level interpretation of visual scenes by labeling each pixel with its corresponding semantic category, as illustrated in \autoref{fig:semantic_segmentation_example}. This capability allows models to not only identify objects but also precisely delineate their shapes and boundaries within an image.

\begin{figure}[htbp]
    \centering
    \setlength{\tabcolsep}{2pt} % adjust spacing
    \renewcommand{\arraystretch}{0.9}
    \fbox{ % Add border around the figure
        % \begin{minipage}{0.7\textwidth}
            \centering
            % CAMs with class labels on the left
            \begin{tabular}{c c} % first column = label

            % Column headers
            (a) Original Image & (b) Segmented Image \\
            [1mm]
            \includegraphics[width=0.3\textwidth,height=0.3\textwidth]{figures/originals/2007_000733.jpg}
            & 
            \includegraphics[width=0.3\textwidth,height=0.3\textwidth]{figures/colored_gts/2007_000733.png}
            \\
            \end{tabular}
        % \end{minipage}
    }
    \caption{Original image and Segmented Image. Two object classes are present in this image: \textit{Person} and \textit{Motorbike}.}
    \label{fig:semantic_segmentation_example}
\end{figure}

The goal of semantic segmentation is to transform raw visual data into structured and meaningful representations that align with human perception. For example, in an autonomous driving scenario, semantic segmentation allows a vehicle to distinguish between roads, pedestrians, vehicles, and traffic signs, ensuring both accurate decision-making and safety. In medical imaging, it aids in the precise delineation of organs, tissues, or pathological regions, thereby assisting in diagnosis and treatment planning. Such applications highlight the importance of fine-grained visual understanding, making semantic segmentation a cornerstone task for achieving scene comprehension in artificial intelligence systems.

Earlier approaches to semantic segmentation primarily depended on manually engineered features combined with classical machine learning algorithms such as random forests, support vector machines (SVMs), and conditional random fields (CRFs). Although these techniques performed reasonably well on certain benchmarks, they struggled to model the complex spatial and contextual dependencies present in natural images. A major turning point occurred with the emergence of deep convolutional neural networks (CNNs), which transformed the field by enabling automatic learning of hierarchical feature representations directly from data. The development of Fully Convolutional Networks (FCNs) was particularly influential, as it replaced traditional fully connected layers with convolutional ones to support dense, pixel-wise prediction. Building on this foundation, subsequent architectures—such as U-Net, SegNet, DeepLab, and PSPNet—introduced innovations like encoder–decoder structures, multi-scale feature aggregation, and attention modules, leading to substantial improvements in segmentation precision and robustness.


Despite these advances, semantic segmentation still faces several challenges. High-quality pixel-level annotations are expensive and time-consuming to obtain, especially for large datasets. Models must also handle issues such as class imbalance, occlusion, scale variation, and boundary precision, which can significantly affect performance. Furthermore, deep models typically require substantial computational resources, making real-time inference a major concern in resource-constrained environments such as mobile or embedded systems. Recent research has thus explored various directions to address these limitations, including weakly supervised, semi-supervised, and unsupervised approaches that reduce the dependency on dense annotations, as well as lightweight architectures optimized for speed and efficiency.

The increasing focus on semantic segmentation has also sparked interest in integrating it with other vision tasks, such as instance and panoptic segmentation, which aim to provide a more comprehensive understanding of the scene by distinguishing individual object instances. Additionally, combining segmentation with temporal and 3D information, as in video and point cloud segmentation, continues to push the boundaries of scene understanding beyond static 2D imagery.

In summary, semantic segmentation represents a vital step toward achieving full scene understanding in computer vision. By providing detailed, structured, and interpretable representations of visual data, it bridges the gap between low-level pixel information and high-level semantic concepts. This research report explores the principles, methodologies, and recent advancements in semantic segmentation, with particular emphasis on deep learning-based approaches and their applications. Furthermore, it discusses the current challenges and potential future directions that could lead to more efficient, generalizable, and human-like visual perception systems.


\subsection{Fully Supervised Semantic Segmentation}
\label{subsec:fully_supervised}
Fully supervised semantic segmentation is the conventional and most extensively utilized framework for training segmentation networks. In this setting, models are trained using datasets that provide detailed pixel-level annotations, where each pixel in an image is manually assigned to a specific semantic category. Such rich supervision enables the model to effectively learn fine-grained spatial structures and contextual relationships among objects and their boundaries. Benchmark datasets like PASCAL VOC \cite{dataset_pascal_voc}, Cityscapes \cite{cityscapes_dataset}, and ADE20K \cite{dataset_ade20k} have been instrumental in driving progress within this paradigm.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=0.9\textwidth]{figures/fsss.pdf}}
    \caption{Pipeline of the fully supervised semantic segmentation framework, where ground-truth pixel-level annotations are used directly to train the segmentation model.}
    \label{fig:fsss}
\end{figure}

In a typical fully supervised setting, a deep convolutional neural network (CNN) or transformer-based architecture learns to map an input image to a dense output map where each pixel corresponds to a predicted semantic label. Models like Fully Convolutional Networks (FCNs), U-Net \cite{fsss_unet}, SegNet \cite{fsss_segnet}, and DeepLab \cite{fsss_deeplabv1} exemplify the success of this paradigm. These models leverage encoder-decoder designs, skip connections, and multi-scale feature extraction to achieve fine-grained segmentation results. The backbone or the encoder captures global context, while the decoder reconstructs spatial details to produce high-resolution predictions. The pipeline is outlined in \autoref{fig:fsss}.

However, fully supervised learning comes with a major limitation—its dependence on large-scale, densely annotated datasets. The annotation process is not only labor-intensive but also time-consuming and costly, particularly for complex scenes with numerous small or overlapping objects. For instance, annotating a single high-resolution image in the Cityscapes dataset~\cite{cityscapes_dataset} can take several hours. This restricts scalability and makes it difficult to apply such methods to specialized domains like medical imaging or remote sensing, where expert labeling is required. Moreover, fully supervised models may overfit to specific dataset distributions, limiting their generalization to unseen environments. Despite these challenges, fully supervised approaches continue to serve as the benchmark and foundation upon which more flexible and efficient paradigms are built.

\subsection{Weakly Supervised Semantic Segmentation}
\label{subsec:weakly_supervised}
To mitigate the annotation burden associated with fully supervised learning, weakly supervised semantic segmentation (WSSS) has emerged as a compelling alternative. Instead of depending on costly pixel-level annotations, weakly supervised approaches train models using simpler forms of supervision, such as image-level tags, bounding boxes, scribbles, or point labels. The primary aim is to maintain competitive segmentation accuracy while greatly reducing the manual labeling effort.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=0.9\textwidth]{figures/wsss.pdf}}
    \caption{Pipeline of the WSSS framework illustrating CAM generation, refinement, and pseudo-label creation for segmentation training. Image-level labels serve as weak supervision to generate Class Activation Maps (CAMs), which are subsequently refined into pseudo-labels for training the segmentation network.}
    \label{fig:wsss}
\end{figure}

In WSSS, the central challenge lies in bridging the gap between coarse supervision and fine-grained pixel-level prediction. For example, when only image-level labels are available (e.g., "cat" or "car"), the model must infer which specific regions of the image correspond to each class. Techniques such as Class Activation Maps (CAMs) and region refinement strategies are commonly employed to identify and expand the most discriminative regions into complete object masks. Additionally, saliency maps, pseudo-label generation, and consistency regularization are often used to improve localization accuracy and enforce smoothness in segmentation outputs. The overall pipeline of this process is outlined in \autoref{fig:wsss}.

Weakly supervised approaches are particularly valuable in domains where annotated data is scarce or costly to obtain, such as biomedical imaging or satellite imagery. They also facilitate large-scale training by leveraging vast amounts of web images or unlabeled data. However, because supervision is less precise, WSSS models may suffer from incomplete object coverage, ambiguous boundaries, or noise in pseudo-labels. To mitigate these issues, hybrid approaches that combine weak supervision with semi-supervised or self-supervised learning are increasingly being explored.

Overall, weakly supervised semantic segmentation represents an important step toward scalable and data-efficient learning. By reducing the dependence on exhaustive manual labeling while maintaining reasonable accuracy, it brings the field closer to the ultimate goal of achieving human-like visual understanding with minimal supervision.

\section{Motivation}
\label{sec:motivation}

Semantic segmentation has established itself as a fundamental task in computer vision, with broad applications in areas such as autonomous driving, medical imaging, and robotics. In these domains, a precise pixel-level understanding of the environment is not merely desirable but often critical: for example, safe navigation of self-driving vehicles relies on reliable scene parsing, and accurate delineation of anatomical structures can directly impact medical diagnosis and treatment.

Despite its importance, conventional semantic segmentation has been heavily reliant on fully supervised learning. This paradigm demands large-scale datasets with dense pixel-level annotations, which are costly and labor-intensive to produce. Annotating high-resolution images can take hours per image, and the process remains prone to human error. Beyond annotation effort, fully supervised methods face additional hurdles: (i) class imbalance in real-world datasets often biases models toward dominant categories, (ii) complex scenes with occlusion, lighting variation, and fine structural detail challenge the robustness of predictions, and (iii) models trained on specific datasets frequently fail to generalize across domains due to dataset-specific biases.

These challenges naturally motivate the exploration of weakly supervised semantic segmentation (WSSS). By replacing dense annotations with weaker forms of supervision—such as image-level labels, bounding boxes, or scribbles—WSSS reduces annotation cost while still enabling model training. The key idea is to leverage weak supervision to generate class activation maps (CAMs) and corresponding pseudo-labels, which can then guide the segmentation process. Although CAMs are often coarse or noisy, refinement strategies allow them to approximate dense ground truth, making WSSS a practical compromise between annotation effort and segmentation performance.

The motivation for this research is thus twofold: first, to address the scalability and practicality issues of fully supervised methods, and second, to improve the quality and reliability of WSSS pipelines. By advancing WSSS, we aim to narrow the performance gap with fully supervised segmentation while ensuring that solutions remain feasible for deployment in diverse and data-constrained real-world scenarios.



\section{Problem Statement}
\label{sec:problem_statement}

Weakly Supervised Semantic Segmentation (WSSS) presents a compelling alternative to fully supervised methods by significantly reducing the dependence on costly pixel-level annotations. However, current WSSS techniques rely on Class Activation Maps (CAMs) \cite{cam} generated from image-level labels to identify object regions. While this approach has shown promise, it often falls short in terms of spatial precision and completeness, leading to suboptimal segmentation results.

In the context of WSSS, the challenge lies in effectively leveraging the image-level labels to produce accurate and detailed segmentation maps. The dependence on CAMs, which are typically generated from global features, can result in sparse and coarse activation maps that fail to capture the fine details of object boundaries. This limitation is particularly pronounced when dealing with complex scenes or occlusions, where precise localization is crucial.

Additionally, the refinement techniques applied to these CAMs often fail to fully leverage the rich affinity information inherent in modern transformer architectures. Consequently, the resulting pseudo-labels lack the spatial precision required for high-quality segmentation, ultimately impacting the overall performance. This highlights the need for more robust backbone architectures capable of effectively capturing both local and global features, as well as advanced CAM refinement strategies to narrow the performance gap between weakly and fully supervised segmentation methods.

So, keeping in mind the above challenges, we aim to develop a weakly supervised semantic segmentation model that can effectively leverage image-level labels to produce accurate and detailed segmentation maps. Our approach will focus on enhancing the spatial precision and completeness of the generated CAMs.

\section{Challenges of WSSS}
\label{sec:challenges_of_wsss}

While weakly supervised semantic segmentation (WSSS) reduces the annotation burden compared to fully supervised methods, it introduces its own set of challenges:

\begin{itemize}
    \item \textbf{Incomplete Object Localization:} Class activation maps (CAMs) derived from image-level labels typically highlight only the most discriminative regions, leaving large portions of the object unlabeled.
    \item \textbf{Noisy Pseudo-labels:} The process of refining CAMs into pixel-level labels often introduces noise and errors, which can propagate during training and degrade performance.
    \item \textbf{Boundary Precision:} Weak supervision lacks explicit boundary cues, making it difficult to segment fine object details and separate adjacent instances accurately.
    \item \textbf{Background Confusion:} Without strong pixel-level supervision, models often misclassify diverse background regions as foreground, or vice versa.
    \item \textbf{Multi-class Co-occurrence:} In scenes containing multiple objects, weak labels struggle to capture clear distinctions, leading to overlapping or missing class activations.
    \item \textbf{Class Imbalance:} Underrepresented classes may not produce strong activations in CAMs, resulting in poor segmentation for rare categories.
    \item \textbf{Dependence on External Cues:} Many WSSS methods rely on saliency maps or additional priors to improve CAMs, but these external cues may be dataset-specific and limit generalization.
\end{itemize}

Addressing these challenges is crucial for advancing WSSS methods toward practical applications where annotation resources are limited.


\section{Contribution}
\label{sec:contribution}

In this work, we make the following key contributions:

\begin{itemize}
    \item We explore the use of the UniCL framework \cite{vl_unicl} with a Swin Transformer backbone \cite{transformer_swin} to enhance CAM generation, leveraging Swin's ability to capture both local fine details and global context through its hierarchical structure.
    \item We adapt the affinity-based CAM refinement technique from WeCLIP \cite{wsss_frozen_clip} to the Swin Transformer backbone by computing pixel affinities from Swin's hierarchical features, allowing the refinement to propagate class activations effectively despite the absence of a global attention map.
    \item We integrate a Pixel-Adaptive Refinement Module (PAR) \cite{wsss_afa_affinity_from_attention} that incorporates both color and spatial information, further refining the pseudo-labels and enhancing boundary accuracy.
    \item We integrate the refined CAMs and Pixel-Adaptive Refinement into the standard WSSS training pipeline, training the model to leverage these improved pseudo-labels for better segmentation performance.

\end{itemize}

\section{Organization}
\label{sec:organization}

The remainder of this thesis report is organized as follows:

\begin{itemize}
    \item \textbf{Chapter \ref{chap:related-works}: Related Works} provides a comprehensive review of existing literature on semantic segmentation. It discusses fully supervised and weakly supervised methods, key architectures like U-Net, DeepLab, and Vision Transformers, and identifies gaps in current research.

    \item \textbf{Chapter \ref{chap:methodology}: Proposed Methodology} details the proposed approach, including the use of the UniCL framework with a Swin Transformer backbone for CAM generation. It also describes the affinity-based CAM refinement technique and the Pixel-Adaptive Refinement Module (PAR) for pseudo-label generation and segmentation refinement.
    \item \textbf{Chapter \ref{chap:results-discussion}: Results and Discussion} presents the experimental setup, quantitative and qualitative analysis, including per-class performance, visualizations of CAMs and segmentation masks, ablation study, interpretation of results, discussion of limitations and challenges, and key findings. Additionally, it covers other explored approaches and insights gained from them.

    \item \textbf{Chapter \ref{chap:conclusion}: Conclusion} summarizes the study’s contributions and major insights, emphasizing the broader implications of the findings. It also outlines potential future research directions.
\end{itemize}

This structure ensures a logical flow, starting from the foundational concepts and related works, progressing through the proposed methodology, and concluding with the results, discussions, and supplementary materials.

