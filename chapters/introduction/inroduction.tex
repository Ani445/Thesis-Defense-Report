\chapter{Introduction}
\label{chap:introduction}

\section{Semantic Segmentation}
\label{sec:semantic_segmentation}
In recent years, the field of computer vision has undergone rapid advancements, largely driven by the increasing availability of large-scale annotated datasets, powerful computational resources, and deep learning architectures. Among the many tasks in computer vision, semantic segmentation has emerged as one of the most critical and challenging problems due to its potential applications in numerous real-world domains, such as autonomous driving, medical imaging, remote sensing, robotics, and augmented reality. Unlike simple image classification, which assigns a single label to an entire image, semantic segmentation aims to provide a pixel-level understanding of the visual scene by assigning a semantic label to every pixel in an image. This process enables machines to perceive not just the presence of objects, but also their exact spatial extents and boundaries within the visual field.

The goal of semantic segmentation is to transform raw visual data into structured and meaningful representations that align with human perception. For example, in an autonomous driving scenario, semantic segmentation allows a vehicle to distinguish between roads, pedestrians, vehicles, and traffic signs, ensuring both accurate decision-making and safety. In medical imaging, it aids in the precise delineation of organs, tissues, or pathological regions, thereby assisting in diagnosis and treatment planning. Such applications highlight the importance of fine-grained visual understanding, making semantic segmentation a cornerstone task for achieving scene comprehension in artificial intelligence systems.

Traditional approaches to semantic segmentation relied heavily on hand-crafted features and classical machine learning methods such as random forests, support vector machines (SVMs), and conditional random fields (CRFs). While these methods achieved moderate success on specific datasets, they were limited by their inability to capture complex spatial and contextual relationships in images. The breakthrough came with the advent of deep convolutional neural networks (CNNs), which revolutionized the field by automatically learning hierarchical feature representations from data. The introduction of architectures such as Fully Convolutional Networks (FCNs) marked a major milestone, replacing fully connected layers with convolutional ones to enable end-to-end pixel-level prediction. Subsequent models, including U-Net, SegNet, DeepLab, and PSPNet, have further refined this capability by incorporating multi-scale feature extraction, encoder-decoder designs, and attention mechanisms to enhance segmentation accuracy and robustness.

Despite these advances, semantic segmentation still faces several challenges. High-quality pixel-level annotations are expensive and time-consuming to obtain, especially for large datasets. Models must also handle issues such as class imbalance, occlusion, scale variation, and boundary precision, which can significantly affect performance. Furthermore, deep models typically require substantial computational resources, making real-time inference a major concern in resource-constrained environments such as mobile or embedded systems. Recent research has thus explored various directions to address these limitations, including weakly supervised, semi-supervised, and unsupervised approaches that reduce the dependency on dense annotations, as well as lightweight architectures optimized for speed and efficiency.

The increasing focus on semantic segmentation has also sparked interest in integrating it with other vision tasks, such as instance and panoptic segmentation, which aim to provide a more comprehensive understanding of the scene by distinguishing individual object instances. Additionally, combining segmentation with temporal and 3D information, as in video and point cloud segmentation, continues to push the boundaries of scene understanding beyond static 2D imagery.

In summary, semantic segmentation represents a vital step toward achieving full scene understanding in computer vision. By providing detailed, structured, and interpretable representations of visual data, it bridges the gap between low-level pixel information and high-level semantic concepts. This research report explores the principles, methodologies, and recent advancements in semantic segmentation, with particular emphasis on deep learning-based approaches and their applications. Furthermore, it discusses the current challenges and potential future directions that could lead to more efficient, generalizable, and human-like visual perception systems.


\subsection{Fully Supervised Semantic Segmentation}
\label{subsec:fully_supervised}
Fully supervised semantic segmentation methods rely on large amounts of labeled data, where each pixel in the training images is annotated with its corresponding class label. These methods typically use deep learning architectures, such as convolutional neural networks (CNNs), to learn complex features and patterns from the labeled data. The performance of these models is heavily dependent on the quality and quantity of the annotated data.

\subsection{Weakly Supervised Semantic Segmentation}
\label{subsec:weakly_supervised}
Weakly supervised semantic segmentation aims to reduce the reliance on extensive pixel-level annotations by utilizing weaker forms of supervision, such as image-level labels, bounding boxes, or scribbles. This approach allows for the training of segmentation models with limited labeled data while leveraging large amounts of unlabeled data. Weakly supervised methods have gained popularity due to their ability to achieve competitive performance with significantly less annotation effort.

\section{Motivation}
\label{sec:motivation}

Semantic segmentation has established itself as a fundamental task in computer vision, with broad applications in areas such as autonomous driving, medical imaging, and robotics. In these domains, a precise pixel-level understanding of the environment is not merely desirable but often critical: for example, safe navigation of self-driving vehicles relies on reliable scene parsing, and accurate delineation of anatomical structures can directly impact medical diagnosis and treatment.

Despite its importance, conventional semantic segmentation has been heavily reliant on fully supervised learning. This paradigm demands large-scale datasets with dense pixel-level annotations, which are costly and labor-intensive to produce. Annotating high-resolution images can take hours per image, and the process remains prone to human error. Beyond annotation effort, fully supervised methods face additional hurdles: (i) class imbalance in real-world datasets often biases models toward dominant categories, (ii) complex scenes with occlusion, lighting variation, and fine structural detail challenge the robustness of predictions, and (iii) models trained on specific datasets frequently fail to generalize across domains due to dataset-specific biases.

These challenges naturally motivate the exploration of weakly supervised semantic segmentation (WSSS). By replacing dense annotations with weaker forms of supervision—such as image-level labels, bounding boxes, or scribbles—WSSS reduces annotation cost while still enabling model training. The key idea is to leverage weak supervision to generate class activation maps (CAMs) and corresponding pseudo-labels, which can then guide the segmentation process. Although CAMs are often coarse or noisy, refinement strategies allow them to approximate dense ground truth, making WSSS a practical compromise between annotation effort and segmentation performance.

The motivation for this research is thus twofold: first, to address the scalability and practicality issues of fully supervised methods, and second, to improve the quality and reliability of WSSS pipelines. By advancing WSSS, we aim to narrow the performance gap with fully supervised segmentation while ensuring that solutions remain feasible for deployment in diverse and data-constrained real-world scenarios.



\section{Problem Statement}
\label{sec:problem_statement}

Weakly Supervised Semantic Segmentation (WSSS) presents a compelling alternative to fully supervised methods by significantly reducing the dependence on costly pixel-level annotations. However, current WSSS techniques rely on Class Activation Maps (CAMs) \cite{cam} generated from image-level labels to identify object regions. While this approach has shown promise, it often falls short in terms of spatial precision and completeness, leading to suboptimal segmentation results.

In the context of WSSS, the challenge lies in effectively leveraging the image-level labels to produce accurate and detailed segmentation maps. The dependence on CAMs, which are typically generated from global features, can result in sparse and coarse activation maps that fail to capture the fine details of object boundaries. This limitation is particularly pronounced when dealing with complex scenes or occlusions, where precise localization is crucial.

Additionally, the refinement techniques applied to these CAMs often fail to fully leverage the rich affinity information inherent in modern transformer architectures. Consequently, the resulting pseudo-labels lack the spatial precision required for high-quality segmentation, ultimately impacting the overall performance. This highlights the need for more robust backbone architectures capable of effectively capturing both local and global features, as well as advanced CAM refinement strategies to narrow the performance gap between weakly and fully supervised segmentation methods.

So, keeping in mind the above challenges, we aim to develop a weakly supervised semantic segmentation model that can effectively leverage image-level labels to produce accurate and detailed segmentation maps. Our approach will focus on enhancing the spatial precision and completeness of the generated CAMs.

\section{Challenges of Semantic Segmentation}
\label{sec:challenges_of_semantic_segmentation}
Semantic segmentation faces several challenges that hinder its widespread adoption and effectiveness:

\section{Challenges of Weakly Supervised Semantic Segmentation}
\label{sec:challenges_of_wsss}

While weakly supervised semantic segmentation (WSSS) reduces the annotation burden compared to fully supervised methods, it introduces its own set of challenges:

\begin{itemize}
    \item \textbf{Incomplete Object Localization:} Class activation maps (CAMs) derived from image-level labels typically highlight only the most discriminative regions, leaving large portions of the object unlabeled.
    \item \textbf{Noisy Pseudo-labels:} The process of refining CAMs into pixel-level labels often introduces noise and errors, which can propagate during training and degrade performance.
    \item \textbf{Boundary Precision:} Weak supervision lacks explicit boundary cues, making it difficult to segment fine object details and separate adjacent instances accurately.
    \item \textbf{Background Confusion:} Without strong pixel-level supervision, models often misclassify diverse background regions as foreground, or vice versa.
    \item \textbf{Multi-class Co-occurrence:} In scenes containing multiple objects, weak labels struggle to capture clear distinctions, leading to overlapping or missing class activations.
    \item \textbf{Class Imbalance:} Underrepresented classes may not produce strong activations in CAMs, resulting in poor segmentation for rare categories.
    \item \textbf{Dependence on External Cues:} Many WSSS methods rely on saliency maps or additional priors to improve CAMs, but these external cues may be dataset-specific and limit generalization.
\end{itemize}

Addressing these challenges is crucial for advancing WSSS methods toward practical applications where annotation resources are limited.


\section{Contribution}
\label{sec:contribution}

In this work, we make the following key contributions:

\begin{itemize}
    \item We explore the use of the UniCL framework \cite{vl_unicl} with a Swin Transformer backbone \cite{transformer_swin} to enhance CAM generation, leveraging Swin's ability to capture both local fine details and global context through its hierarchical structure.
    \item We adapt the affinity-based CAM refinement technique from WeCLIP \cite{wsss_frozen_clip} to the Swin Transformer backbone by computing pixel affinities from Swin's hierarchical features, allowing the refinement to propagate class activations effectively despite the absence of a global attention map.
    \item We integrate a Pixel-Adaptive Refinement Module (PAR) \cite{wsss_afa_affinity_from_attention} that incorporates both color and spatial information, further refining the pseudo-labels and enhancing boundary accuracy.
    \item We integrate the refined CAMs and Pixel-Adaptive Refinement into the standard WSSS training pipeline, training the model to leverage these improved pseudo-labels for better segmentation performance.

\end{itemize}

\section{Organization}
\label{sec:organization}

The remainder of this thesis report is organized as follows:

\begin{itemize}
    \item \textbf{Chapter \ref{chap:related-works}: Related Works} provides a comprehensive review of existing literature on semantic segmentation. It discusses fully supervised and weakly supervised methods, key architectures like U-Net, DeepLab, and Vision Transformers, and identifies gaps in current research.

    \item \textbf{Chapter \ref{chap:methodology}: Proposed Methodology} details the proposed approach, including the use of the UniCL framework with a Swin Transformer backbone for CAM generation. It also describes the affinity-based CAM refinement technique and the Pixel-Adaptive Refinement Module (PAR) for pseudo-label generation and segmentation refinement.
\end{itemize}

This structure ensures a logical flow, starting from the foundational concepts and related works, progressing through the proposed methodology, and concluding with the results, discussions, and supplementary materials.

